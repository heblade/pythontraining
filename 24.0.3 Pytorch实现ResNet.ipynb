{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通过PyTorch手动实现ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet网络是参考了VGG19网络，在其基础上进行了修改，并通过短路机制加入了残差单元，如图所示。变化主要体现在ResNet直接使用stride=2的卷积做下采样，并且用global average pool层替换了全连接层。\n",
    "\n",
    "ResNet的一个重要设计原则是：当feature map大小降低一半时，feature map的数量增加一倍，这保持了网络层的复杂度。\n",
    "\n",
    "从图中可以看到，ResNet相比普通网络每两层间增加了短路机制，这就形成了残差学习，其中虚线表示feature map数量发生了改变。\n",
    "\n",
    "图中展示的34-layer的ResNet，还可以构建更深的网络如表1所示。从表中可以看到，对于18-layer和34-layer的ResNet，其进行的两层间的残差学习，当网络更深时，其进行的是三层间的残差学习，三层卷积核分别是1x1，3x3和1x1，一个值得注意的是隐含层的feature map数量是比较小的，并且是输出feature map数量的1/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/resnet.jpg\"  alt=\"ResNet网络结构\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "图中展示的34-layer的ResNet，还可以构建更深的网络如下表所示。从表中可以看到，对于18-layer和34-layer的ResNet，其进行的两层间的残差学习，当网络更深时，其进行的是三层间的残差学习，三层卷积核分别是1x1，3x3和1x1，一个值得注意的是隐含层的feature map数量是比较小的，并且是输出feature map数量的1/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/resnet_2.jpg\"  alt=\"ResNet表格\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们再分析一下残差单元，ResNet使用两种残差单元，如下图所示。\n",
    "\n",
    "左图对应的是浅层网络，而右图对应的是深层网络。\n",
    "\n",
    "对于短路连接，当输入和输出维度一致时，可以直接将输入加到输出上。但是当维度不一致时（对应的是维度增加一倍），这就不能直接相加。\n",
    "\n",
    "有两种策略：\n",
    "\n",
    "（1）采用zero-padding增加维度，此时一般要先做一个downsamp，可以采用strde=2的pooling，这样不会增加参数；\n",
    "\n",
    "（2）采用新的映射（projection shortcut），一般采用1x1的卷积，这样会增加参数，也会增加计算量。\n",
    "\n",
    "短路连接除了直接使用恒等映射，当然都可以采用projection shortcut。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image/resnet_3.png\"  alt=\"ResNet shortcut\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T03:29:42.042830Z",
     "start_time": "2023-01-09T03:29:41.054805Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T03:29:45.335461Z",
     "start_time": "2023-01-09T03:29:44.339784Z"
    }
   },
   "outputs": [],
   "source": [
    "# device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T03:29:45.351430Z",
     "start_time": "2023-01-09T03:29:45.337600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T03:29:46.053521Z",
     "start_time": "2023-01-09T03:29:46.049598Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "EPOCHS = 20\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T03:29:47.167070Z",
     "start_time": "2023-01-09T03:29:47.151072Z"
    }
   },
   "outputs": [],
   "source": [
    "# Image preprocessing modules\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T03:30:08.429584Z",
     "start_time": "2023-01-09T03:30:06.030581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./cifar10_data\\cifar-10-python.tar.gz\n",
      "Extracting ./cifar10_data\\cifar-10-python.tar.gz to ./cifar10_data\n"
     ]
    }
   ],
   "source": [
    "# cifar10 32*32*3\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./cifar10_data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./cifar10_data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T03:30:27.192719Z",
     "start_time": "2023-01-09T03:30:27.186261Z"
    }
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T03:30:30.092911Z",
     "start_time": "2023-01-09T03:30:30.088795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T03:30:31.645657Z",
     "start_time": "2023-01-09T03:30:31.635649Z"
    }
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T03:30:38.418030Z",
     "start_time": "2023-01-09T03:30:38.399028Z"
    }
   },
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T03:30:39.194785Z",
     "start_time": "2023-01-09T03:30:39.179775Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T03:30:40.241683Z",
     "start_time": "2023-01-09T03:30:40.230603Z"
    }
   },
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T03:32:38.075083Z",
     "start_time": "2023-01-09T03:32:37.772570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "test_net_50 = ResNet50().to(device)\n",
    "y_50 = test_net_50(torch.randn(1, 3, 32, 32).to(device))\n",
    "print(y_50.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T03:32:45.758625Z",
     "start_time": "2023-01-09T03:32:45.719051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 64, 32, 32]           4,096\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
      "            Conv2d-7          [-1, 256, 32, 32]          16,384\n",
      "       BatchNorm2d-8          [-1, 256, 32, 32]             512\n",
      "            Conv2d-9          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-10          [-1, 256, 32, 32]             512\n",
      "       Bottleneck-11          [-1, 256, 32, 32]               0\n",
      "           Conv2d-12           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-13           [-1, 64, 32, 32]             128\n",
      "           Conv2d-14           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 32, 32]             128\n",
      "           Conv2d-16          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-17          [-1, 256, 32, 32]             512\n",
      "       Bottleneck-18          [-1, 256, 32, 32]               0\n",
      "           Conv2d-19           [-1, 64, 32, 32]          16,384\n",
      "      BatchNorm2d-20           [-1, 64, 32, 32]             128\n",
      "           Conv2d-21           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-22           [-1, 64, 32, 32]             128\n",
      "           Conv2d-23          [-1, 256, 32, 32]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 32, 32]             512\n",
      "       Bottleneck-25          [-1, 256, 32, 32]               0\n",
      "           Conv2d-26          [-1, 128, 32, 32]          32,768\n",
      "      BatchNorm2d-27          [-1, 128, 32, 32]             256\n",
      "           Conv2d-28          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 16, 16]             256\n",
      "           Conv2d-30          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-31          [-1, 512, 16, 16]           1,024\n",
      "           Conv2d-32          [-1, 512, 16, 16]         131,072\n",
      "      BatchNorm2d-33          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-34          [-1, 512, 16, 16]               0\n",
      "           Conv2d-35          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-36          [-1, 128, 16, 16]             256\n",
      "           Conv2d-37          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-38          [-1, 128, 16, 16]             256\n",
      "           Conv2d-39          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-40          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-41          [-1, 512, 16, 16]               0\n",
      "           Conv2d-42          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-43          [-1, 128, 16, 16]             256\n",
      "           Conv2d-44          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-45          [-1, 128, 16, 16]             256\n",
      "           Conv2d-46          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-47          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-48          [-1, 512, 16, 16]               0\n",
      "           Conv2d-49          [-1, 128, 16, 16]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 16, 16]             256\n",
      "           Conv2d-51          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-52          [-1, 128, 16, 16]             256\n",
      "           Conv2d-53          [-1, 512, 16, 16]          65,536\n",
      "      BatchNorm2d-54          [-1, 512, 16, 16]           1,024\n",
      "       Bottleneck-55          [-1, 512, 16, 16]               0\n",
      "           Conv2d-56          [-1, 256, 16, 16]         131,072\n",
      "      BatchNorm2d-57          [-1, 256, 16, 16]             512\n",
      "           Conv2d-58            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-59            [-1, 256, 8, 8]             512\n",
      "           Conv2d-60           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-61           [-1, 1024, 8, 8]           2,048\n",
      "           Conv2d-62           [-1, 1024, 8, 8]         524,288\n",
      "      BatchNorm2d-63           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-64           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-65            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-66            [-1, 256, 8, 8]             512\n",
      "           Conv2d-67            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-68            [-1, 256, 8, 8]             512\n",
      "           Conv2d-69           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-70           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-71           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-72            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-73            [-1, 256, 8, 8]             512\n",
      "           Conv2d-74            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-75            [-1, 256, 8, 8]             512\n",
      "           Conv2d-76           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-77           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-78           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-79            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-80            [-1, 256, 8, 8]             512\n",
      "           Conv2d-81            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-82            [-1, 256, 8, 8]             512\n",
      "           Conv2d-83           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-84           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-85           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-86            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-87            [-1, 256, 8, 8]             512\n",
      "           Conv2d-88            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-89            [-1, 256, 8, 8]             512\n",
      "           Conv2d-90           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-91           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-92           [-1, 1024, 8, 8]               0\n",
      "           Conv2d-93            [-1, 256, 8, 8]         262,144\n",
      "      BatchNorm2d-94            [-1, 256, 8, 8]             512\n",
      "           Conv2d-95            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-96            [-1, 256, 8, 8]             512\n",
      "           Conv2d-97           [-1, 1024, 8, 8]         262,144\n",
      "      BatchNorm2d-98           [-1, 1024, 8, 8]           2,048\n",
      "       Bottleneck-99           [-1, 1024, 8, 8]               0\n",
      "          Conv2d-100            [-1, 512, 8, 8]         524,288\n",
      "     BatchNorm2d-101            [-1, 512, 8, 8]           1,024\n",
      "          Conv2d-102            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-103            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-104           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-105           [-1, 2048, 4, 4]           4,096\n",
      "          Conv2d-106           [-1, 2048, 4, 4]       2,097,152\n",
      "     BatchNorm2d-107           [-1, 2048, 4, 4]           4,096\n",
      "      Bottleneck-108           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-109            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-110            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-111            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-112            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-113           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-114           [-1, 2048, 4, 4]           4,096\n",
      "      Bottleneck-115           [-1, 2048, 4, 4]               0\n",
      "          Conv2d-116            [-1, 512, 4, 4]       1,048,576\n",
      "     BatchNorm2d-117            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-118            [-1, 512, 4, 4]       2,359,296\n",
      "     BatchNorm2d-119            [-1, 512, 4, 4]           1,024\n",
      "          Conv2d-120           [-1, 2048, 4, 4]       1,048,576\n",
      "     BatchNorm2d-121           [-1, 2048, 4, 4]           4,096\n",
      "      Bottleneck-122           [-1, 2048, 4, 4]               0\n",
      "          Linear-123                   [-1, 10]          20,490\n",
      "================================================================\n",
      "Total params: 23,520,842\n",
      "Trainable params: 23,520,842\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 66.13\n",
      "Params size (MB): 89.72\n",
      "Estimated Total Size (MB): 155.86\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(test_net_50, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T03:30:45.601775Z",
     "start_time": "2023-01-09T03:30:41.399169Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "test_net_18 = ResNet18().to(device)\n",
    "y = test_net_18(torch.randn(1, 3, 32, 32).to(device))\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T03:30:45.632559Z",
     "start_time": "2023-01-09T03:30:45.603768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
      "        BasicBlock-7           [-1, 64, 32, 32]               0\n",
      "            Conv2d-8           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
      "           Conv2d-10           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
      "       BasicBlock-12           [-1, 64, 32, 32]               0\n",
      "           Conv2d-13          [-1, 128, 16, 16]          73,728\n",
      "      BatchNorm2d-14          [-1, 128, 16, 16]             256\n",
      "           Conv2d-15          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-16          [-1, 128, 16, 16]             256\n",
      "           Conv2d-17          [-1, 128, 16, 16]           8,192\n",
      "      BatchNorm2d-18          [-1, 128, 16, 16]             256\n",
      "       BasicBlock-19          [-1, 128, 16, 16]               0\n",
      "           Conv2d-20          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-21          [-1, 128, 16, 16]             256\n",
      "           Conv2d-22          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 16, 16]             256\n",
      "       BasicBlock-24          [-1, 128, 16, 16]               0\n",
      "           Conv2d-25            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-26            [-1, 256, 8, 8]             512\n",
      "           Conv2d-27            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-28            [-1, 256, 8, 8]             512\n",
      "           Conv2d-29            [-1, 256, 8, 8]          32,768\n",
      "      BatchNorm2d-30            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-31            [-1, 256, 8, 8]               0\n",
      "           Conv2d-32            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-33            [-1, 256, 8, 8]             512\n",
      "           Conv2d-34            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-35            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-36            [-1, 256, 8, 8]               0\n",
      "           Conv2d-37            [-1, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-38            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-39            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-40            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-41            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-42            [-1, 512, 4, 4]           1,024\n",
      "       BasicBlock-43            [-1, 512, 4, 4]               0\n",
      "           Conv2d-44            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-45            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-46            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-47            [-1, 512, 4, 4]           1,024\n",
      "       BasicBlock-48            [-1, 512, 4, 4]               0\n",
      "           Linear-49                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,173,962\n",
      "Trainable params: 11,173,962\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 11.25\n",
      "Params size (MB): 42.63\n",
      "Estimated Total Size (MB): 53.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(test_net_18, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T03:30:51.453008Z",
     "start_time": "2023-01-09T03:30:51.317594Z"
    }
   },
   "outputs": [],
   "source": [
    "net18_model = ResNet18().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T03:30:52.980748Z",
     "start_time": "2023-01-09T03:30:52.964479Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net18_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-09T03:30:54.275986Z",
     "start_time": "2023-01-09T03:30:54.268987Z"
    }
   },
   "outputs": [],
   "source": [
    "# for updating learning rate\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/20, step:100/782, Loss:1.978853, Acc: 26.125000, (correct:1672/total:6400)\n",
      "Epoch:1/20, step:200/782, Loss:1.855718, Acc: 30.453125, (correct:3898/total:12800)\n",
      "Epoch:1/20, step:300/782, Loss:1.768279, Acc: 33.708333, (correct:6472/total:19200)\n",
      "Epoch:1/20, step:400/782, Loss:1.693645, Acc: 36.714844, (correct:9399/total:25600)\n",
      "Epoch:1/20, step:500/782, Loss:1.626807, Acc: 39.409375, (correct:12611/total:32000)\n",
      "Epoch:1/20, step:600/782, Loss:1.571510, Acc: 41.812500, (correct:16056/total:38400)\n",
      "Epoch:1/20, step:700/782, Loss:1.518394, Acc: 43.783482, (correct:19615/total:44800)\n",
      "Epoch:2/20, step:100/782, Loss:12.717088, Acc: 46.849291, (correct:26423/total:56400)\n",
      "Epoch:2/20, step:200/782, Loss:6.900116, Acc: 48.367834, (correct:30375/total:62800)\n",
      "Epoch:2/20, step:300/782, Loss:4.951311, Acc: 49.696532, (correct:34390/total:69200)\n",
      "Epoch:2/20, step:400/782, Loss:3.964879, Acc: 50.870370, (correct:38458/total:75600)\n",
      "Epoch:2/20, step:500/782, Loss:3.370199, Acc: 51.953659, (correct:42602/total:82000)\n",
      "Epoch:2/20, step:600/782, Loss:2.968631, Acc: 52.970588, (correct:46826/total:88400)\n",
      "Epoch:2/20, step:700/782, Loss:2.678861, Acc: 53.864979, (correct:51064/total:94800)\n",
      "Epoch:3/20, step:100/782, Loss:20.394484, Acc: 55.396617, (correct:58942/total:106400)\n",
      "Epoch:3/20, step:200/782, Loss:10.631215, Acc: 56.169326, (correct:63359/total:112800)\n",
      "Epoch:3/20, step:300/782, Loss:7.365597, Acc: 56.919463, (correct:67848/total:119200)\n",
      "Epoch:3/20, step:400/782, Loss:5.727721, Acc: 57.641720, (correct:72398/total:125600)\n",
      "Epoch:3/20, step:500/782, Loss:4.738761, Acc: 58.343939, (correct:77014/total:132000)\n",
      "Epoch:3/20, step:600/782, Loss:4.080612, Acc: 58.977601, (correct:81625/total:138400)\n",
      "Epoch:3/20, step:700/782, Loss:3.605811, Acc: 59.634669, (correct:86351/total:144800)\n",
      "Epoch:4/20, step:100/782, Loss:26.529359, Acc: 60.813299, (correct:95112/total:156400)\n",
      "Epoch:4/20, step:200/782, Loss:13.610982, Acc: 61.385749, (correct:99936/total:162800)\n",
      "Epoch:4/20, step:300/782, Loss:9.305336, Acc: 61.911348, (correct:104754/total:169200)\n",
      "Epoch:4/20, step:400/782, Loss:7.152050, Acc: 62.421412, (correct:109612/total:175600)\n",
      "Epoch:4/20, step:500/782, Loss:5.853482, Acc: 62.936264, (correct:114544/total:182000)\n",
      "Epoch:4/20, step:600/782, Loss:4.986006, Acc: 63.424098, (correct:119491/total:188400)\n",
      "Epoch:4/20, step:700/782, Loss:4.364814, Acc: 63.903491, (correct:124484/total:194800)\n",
      "Epoch:5/20, step:100/782, Loss:31.646501, Acc: 64.745640, (correct:133635/total:206400)\n",
      "Epoch:5/20, step:200/782, Loss:16.122930, Acc: 65.175282, (correct:138693/total:212800)\n",
      "Epoch:5/20, step:300/782, Loss:10.933599, Acc: 65.624544, (correct:143849/total:219200)\n",
      "Epoch:5/20, step:400/782, Loss:8.343760, Acc: 66.040780, (correct:148988/total:225600)\n",
      "Epoch:5/20, step:500/782, Loss:6.789430, Acc: 66.424569, (correct:154105/total:232000)\n",
      "Epoch:5/20, step:600/782, Loss:5.753928, Acc: 66.782718, (correct:159210/total:238400)\n",
      "Epoch:5/20, step:700/782, Loss:5.015164, Acc: 67.122141, (correct:164315/total:244800)\n",
      "Epoch:6/20, step:100/782, Loss:36.076850, Acc: 67.762090, (correct:173742/total:256400)\n",
      "Epoch:6/20, step:200/782, Loss:18.290857, Acc: 68.120624, (correct:179021/total:262800)\n",
      "Epoch:6/20, step:300/782, Loss:12.367687, Acc: 68.456909, (correct:184286/total:269200)\n",
      "Epoch:6/20, step:400/782, Loss:9.410291, Acc: 68.755806, (correct:189491/total:275600)\n",
      "Epoch:6/20, step:500/782, Loss:7.629159, Acc: 69.071277, (correct:194781/total:282000)\n",
      "Epoch:6/20, step:600/782, Loss:6.432442, Acc: 69.414355, (correct:200191/total:288400)\n",
      "Epoch:6/20, step:700/782, Loss:5.581423, Acc: 69.721506, (correct:205539/total:294800)\n",
      "Epoch:7/20, step:100/782, Loss:39.969665, Acc: 70.242820, (correct:215224/total:306400)\n",
      "Epoch:7/20, step:200/782, Loss:20.205380, Acc: 70.534527, (correct:220632/total:312800)\n",
      "Epoch:7/20, step:300/782, Loss:13.617728, Acc: 70.822055, (correct:226064/total:319200)\n",
      "Epoch:7/20, step:400/782, Loss:10.325080, Acc: 71.094287, (correct:231483/total:325600)\n",
      "Epoch:7/20, step:500/782, Loss:8.349436, Acc: 71.359337, (correct:236913/total:332000)\n",
      "Epoch:7/20, step:600/782, Loss:7.033880, Acc: 71.602541, (correct:242303/total:338400)\n",
      "Epoch:7/20, step:700/782, Loss:6.093602, Acc: 71.842517, (correct:247713/total:344800)\n",
      "Epoch:8/20, step:100/782, Loss:43.409082, Acc: 72.285073, (correct:257624/total:356400)\n",
      "Epoch:8/20, step:200/782, Loss:21.908947, Acc: 72.521499, (correct:263108/total:362800)\n",
      "Epoch:8/20, step:300/782, Loss:14.740397, Acc: 72.748646, (correct:268588/total:369200)\n",
      "Epoch:8/20, step:400/782, Loss:11.161836, Acc: 72.968051, (correct:274068/total:375600)\n",
      "Epoch:8/20, step:500/782, Loss:9.012490, Acc: 73.184555, (correct:279565/total:382000)\n",
      "Epoch:8/20, step:600/782, Loss:7.576155, Acc: 73.393924, (correct:285062/total:388400)\n",
      "Epoch:8/20, step:700/782, Loss:6.550729, Acc: 73.603597, (correct:290587/total:394800)\n",
      "Epoch:9/20, step:100/782, Loss:46.567477, Acc: 73.969242, (correct:300611/total:406400)\n",
      "Epoch:9/20, step:200/782, Loss:23.464126, Acc: 74.176841, (correct:306202/total:412800)\n",
      "Epoch:9/20, step:300/782, Loss:15.766542, Acc: 74.372376, (correct:311769/total:419200)\n",
      "Epoch:9/20, step:400/782, Loss:11.917766, Acc: 74.557801, (correct:317318/total:425600)\n",
      "Epoch:9/20, step:500/782, Loss:9.606420, Acc: 74.754861, (correct:322941/total:432000)\n",
      "Epoch:9/20, step:600/782, Loss:8.065711, Acc: 74.938869, (correct:328532/total:438400)\n",
      "Epoch:9/20, step:700/782, Loss:6.969429, Acc: 75.102743, (correct:334057/total:444800)\n",
      "Epoch:10/20, step:100/782, Loss:49.412595, Acc: 75.425285, (correct:344241/total:456400)\n",
      "Epoch:10/20, step:200/782, Loss:24.872848, Acc: 75.609334, (correct:349920/total:462800)\n",
      "Epoch:10/20, step:300/782, Loss:16.689661, Acc: 75.786019, (correct:355588/total:469200)\n",
      "Epoch:10/20, step:400/782, Loss:12.604191, Acc: 75.945963, (correct:361199/total:475600)\n",
      "Epoch:10/20, step:500/782, Loss:10.150655, Acc: 76.110166, (correct:366851/total:482000)\n",
      "Epoch:10/20, step:600/782, Loss:8.519491, Acc: 76.263718, (correct:372472/total:488400)\n",
      "Epoch:10/20, step:700/782, Loss:7.352667, Acc: 76.418957, (correct:378121/total:494800)\n",
      "Epoch:11/20, step:100/782, Loss:52.045115, Acc: 76.710703, (correct:388463/total:506400)\n",
      "Epoch:11/20, step:200/782, Loss:26.180413, Acc: 76.868175, (correct:394180/total:512800)\n",
      "Epoch:11/20, step:300/782, Loss:17.551890, Acc: 77.030431, (correct:399942/total:519200)\n",
      "Epoch:11/20, step:400/782, Loss:13.242270, Acc: 77.174087, (correct:405627/total:525600)\n",
      "Epoch:11/20, step:500/782, Loss:10.655485, Acc: 77.315789, (correct:411320/total:532000)\n",
      "Epoch:11/20, step:600/782, Loss:8.932615, Acc: 77.454123, (correct:417013/total:538400)\n",
      "Epoch:11/20, step:700/782, Loss:7.701520, Acc: 77.589758, (correct:422709/total:544800)\n",
      "Epoch:12/20, step:100/782, Loss:54.461563, Acc: 77.839684, (correct:433100/total:556400)\n",
      "Epoch:12/20, step:200/782, Loss:27.377216, Acc: 77.976724, (correct:438853/total:562800)\n",
      "Epoch:12/20, step:300/782, Loss:18.341947, Acc: 78.119466, (correct:444656/total:569200)\n",
      "Epoch:12/20, step:400/782, Loss:13.828052, Acc: 78.252606, (correct:450422/total:575600)\n",
      "Epoch:12/20, step:500/782, Loss:11.118384, Acc: 78.382646, (correct:456187/total:582000)\n",
      "Epoch:12/20, step:600/782, Loss:9.310720, Acc: 78.514616, (correct:461980/total:588400)\n",
      "Epoch:12/20, step:700/782, Loss:8.021160, Acc: 78.638366, (correct:467741/total:594800)\n",
      "Epoch:13/20, step:100/782, Loss:56.640643, Acc: 78.865270, (correct:478239/total:606400)\n",
      "Epoch:13/20, step:200/782, Loss:28.450599, Acc: 78.991678, (correct:484061/total:612800)\n",
      "Epoch:13/20, step:300/782, Loss:19.051631, Acc: 79.114987, (correct:489880/total:619200)\n",
      "Epoch:13/20, step:400/782, Loss:14.354404, Acc: 79.236893, (correct:495706/total:625600)\n",
      "Epoch:13/20, step:500/782, Loss:11.535423, Acc: 79.355854, (correct:501529/total:632000)\n",
      "Epoch:13/20, step:600/782, Loss:9.656468, Acc: 79.469612, (correct:507334/total:638400)\n",
      "Epoch:13/20, step:700/782, Loss:8.313712, Acc: 79.578784, (correct:513124/total:644800)\n",
      "Epoch:14/20, step:100/782, Loss:58.660582, Acc: 79.780774, (correct:523681/total:656400)\n",
      "Epoch:14/20, step:200/782, Loss:29.448005, Acc: 79.893935, (correct:529537/total:662800)\n",
      "Epoch:14/20, step:300/782, Loss:19.712035, Acc: 80.007472, (correct:535410/total:669200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:14/20, step:400/782, Loss:14.846225, Acc: 80.115897, (correct:541263/total:675600)\n",
      "Epoch:14/20, step:500/782, Loss:11.925484, Acc: 80.225220, (correct:547136/total:682000)\n",
      "Epoch:14/20, step:600/782, Loss:9.978239, Acc: 80.333236, (correct:553014/total:688400)\n",
      "Epoch:14/20, step:700/782, Loss:8.586854, Acc: 80.435809, (correct:558868/total:694800)\n",
      "Epoch:15/20, step:100/782, Loss:60.538881, Acc: 80.624292, (correct:569530/total:706400)\n",
      "Epoch:15/20, step:200/782, Loss:30.373648, Acc: 80.731762, (correct:575456/total:712800)\n",
      "Epoch:15/20, step:300/782, Loss:20.325100, Acc: 80.831479, (correct:581340/total:719200)\n",
      "Epoch:15/20, step:400/782, Loss:15.297526, Acc: 80.930402, (correct:587231/total:725600)\n",
      "Epoch:15/20, step:500/782, Loss:12.282547, Acc: 81.028689, (correct:593130/total:732000)\n",
      "Epoch:15/20, step:600/782, Loss:10.272424, Acc: 81.122291, (correct:599007/total:738400)\n",
      "Epoch:15/20, step:700/782, Loss:8.836927, Acc: 81.218448, (correct:604915/total:744800)\n",
      "Epoch:16/20, step:100/782, Loss:62.254729, Acc: 81.390799, (correct:615640/total:756400)\n",
      "Epoch:16/20, step:200/782, Loss:31.232282, Acc: 81.485711, (correct:621573/total:762800)\n",
      "Epoch:16/20, step:300/782, Loss:20.890125, Acc: 81.579563, (correct:627510/total:769200)\n",
      "Epoch:16/20, step:400/782, Loss:15.720308, Acc: 81.671867, (correct:633447/total:775600)\n",
      "Epoch:16/20, step:500/782, Loss:12.617125, Acc: 81.762276, (correct:639381/total:782000)\n",
      "Epoch:16/20, step:600/782, Loss:10.548198, Acc: 81.853120, (correct:645330/total:788400)\n",
      "Epoch:16/20, step:700/782, Loss:9.071481, Acc: 81.940614, (correct:651264/total:794800)\n",
      "Epoch:17/20, step:100/782, Loss:63.872659, Acc: 82.095734, (correct:662020/total:806400)\n",
      "Epoch:17/20, step:200/782, Loss:32.021351, Acc: 82.191929, (correct:668056/total:812800)\n",
      "Epoch:17/20, step:300/782, Loss:21.402871, Acc: 82.283691, (correct:674068/total:819200)\n",
      "Epoch:17/20, step:400/782, Loss:16.100517, Acc: 82.368702, (correct:680036/total:825600)\n",
      "Epoch:17/20, step:500/782, Loss:12.924718, Acc: 82.446034, (correct:685951/total:832000)\n",
      "Epoch:17/20, step:600/782, Loss:10.802792, Acc: 82.529819, (correct:691930/total:838400)\n",
      "Epoch:17/20, step:700/782, Loss:9.289166, Acc: 82.606416, (correct:697859/total:844800)\n",
      "Epoch:18/20, step:100/782, Loss:65.350834, Acc: 82.756889, (correct:708730/total:856400)\n",
      "Epoch:18/20, step:200/782, Loss:32.764581, Acc: 82.837854, (correct:714725/total:862800)\n",
      "Epoch:18/20, step:300/782, Loss:21.901908, Acc: 82.920156, (correct:720742/total:869200)\n",
      "Epoch:18/20, step:400/782, Loss:16.471322, Acc: 82.997031, (correct:726722/total:875600)\n",
      "Epoch:18/20, step:500/782, Loss:13.210223, Acc: 83.077664, (correct:732745/total:882000)\n",
      "Epoch:18/20, step:600/782, Loss:11.038163, Acc: 83.154998, (correct:738749/total:888400)\n",
      "Epoch:18/20, step:700/782, Loss:9.488618, Acc: 83.230554, (correct:744747/total:894800)\n",
      "Epoch:19/20, step:100/782, Loss:66.728164, Acc: 83.369153, (correct:755658/total:906400)\n",
      "Epoch:19/20, step:200/782, Loss:33.441299, Acc: 83.449167, (correct:761724/total:912800)\n",
      "Epoch:19/20, step:300/782, Loss:22.351163, Acc: 83.522846, (correct:767742/total:919200)\n",
      "Epoch:19/20, step:400/782, Loss:16.803200, Acc: 83.597774, (correct:773781/total:925600)\n",
      "Epoch:19/20, step:500/782, Loss:13.476087, Acc: 83.668884, (correct:779794/total:932000)\n",
      "Epoch:19/20, step:600/782, Loss:11.256550, Acc: 83.740622, (correct:785822/total:938400)\n",
      "Epoch:19/20, step:700/782, Loss:9.671412, Acc: 83.812659, (correct:791862/total:944800)\n",
      "Epoch:20/20, step:100/782, Loss:67.964640, Acc: 83.947616, (correct:802875/total:956400)\n",
      "Epoch:20/20, step:200/782, Loss:34.057659, Acc: 84.017761, (correct:808923/total:962800)\n",
      "Epoch:20/20, step:300/782, Loss:22.756348, Acc: 84.086257, (correct:814964/total:969200)\n",
      "Epoch:20/20, step:400/782, Loss:17.106435, Acc: 84.157134, (correct:821037/total:975600)\n",
      "Epoch:20/20, step:500/782, Loss:13.714564, Acc: 84.226884, (correct:827108/total:982000)\n",
      "Epoch:20/20, step:600/782, Loss:11.456538, Acc: 84.289660, (correct:833119/total:988400)\n",
      "Epoch:20/20, step:700/782, Loss:9.843305, Acc: 84.353036, (correct:839144/total:994800)\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "pre_epoch_total_step = len(trainloader)\n",
    "current_lr = learning_rate\n",
    "net18_model.train()\n",
    "train_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, (x, y) in enumerate(trainloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # forward\n",
    "        prediction = net18_model(x)\n",
    "        loss = criterion(prediction, y)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = prediction.max(1)\n",
    "        total += y.size(0)\n",
    "        correct += predicted.eq(y).sum().item()\n",
    "        \n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            template = r\"Epoch:{}/{}, step:{}/{}, Loss:{:.6f}, Acc: {:.6f}, (correct:{}/total:{})\"\n",
    "            print(template.format(epoch+1,\n",
    "                                  EPOCHS, \n",
    "                                  i+1, \n",
    "                                  pre_epoch_total_step, \n",
    "                                  train_loss/(i+1),\n",
    "                                  100.*correct/total,\n",
    "                                 correct,\n",
    "                                 total))\n",
    "\n",
    "    # decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        current_lr = current_lr/2\n",
    "        update_lr(optimizer, current_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:90.38%\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "net18_model.eval()\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for x, y in testloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        prediction = net18_model(x)\n",
    "        _, predic = torch.max(prediction.data, dim=1)\n",
    "        total += y.size(0)\n",
    "        correct += (predic == y).sum().item()\n",
    "\n",
    "    print(\"Accuracy:{}%\".format(100 * correct / total))\n",
    "\n",
    "model_path = r'./cifar10_data/model/'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "model_name = os.path.join(model_path, 'cifar10_resnet.ckpt')\n",
    "# save model\n",
    "torch.save(net18_model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通过PyTorch手动实现ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "EPOCHS = 20\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing modules\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# cifar10 32*32*3\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./cifar10_data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./cifar10_data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "test_net_18 = ResNet18().to(device)\n",
    "y = test_net_18(torch.randn(1, 3, 32, 32).to(device))\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "            Conv2d-3           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
      "        BasicBlock-7           [-1, 64, 32, 32]               0\n",
      "            Conv2d-8           [-1, 64, 32, 32]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
      "           Conv2d-10           [-1, 64, 32, 32]          36,864\n",
      "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
      "       BasicBlock-12           [-1, 64, 32, 32]               0\n",
      "           Conv2d-13          [-1, 128, 16, 16]          73,728\n",
      "      BatchNorm2d-14          [-1, 128, 16, 16]             256\n",
      "           Conv2d-15          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-16          [-1, 128, 16, 16]             256\n",
      "           Conv2d-17          [-1, 128, 16, 16]           8,192\n",
      "      BatchNorm2d-18          [-1, 128, 16, 16]             256\n",
      "       BasicBlock-19          [-1, 128, 16, 16]               0\n",
      "           Conv2d-20          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-21          [-1, 128, 16, 16]             256\n",
      "           Conv2d-22          [-1, 128, 16, 16]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 16, 16]             256\n",
      "       BasicBlock-24          [-1, 128, 16, 16]               0\n",
      "           Conv2d-25            [-1, 256, 8, 8]         294,912\n",
      "      BatchNorm2d-26            [-1, 256, 8, 8]             512\n",
      "           Conv2d-27            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-28            [-1, 256, 8, 8]             512\n",
      "           Conv2d-29            [-1, 256, 8, 8]          32,768\n",
      "      BatchNorm2d-30            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-31            [-1, 256, 8, 8]               0\n",
      "           Conv2d-32            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-33            [-1, 256, 8, 8]             512\n",
      "           Conv2d-34            [-1, 256, 8, 8]         589,824\n",
      "      BatchNorm2d-35            [-1, 256, 8, 8]             512\n",
      "       BasicBlock-36            [-1, 256, 8, 8]               0\n",
      "           Conv2d-37            [-1, 512, 4, 4]       1,179,648\n",
      "      BatchNorm2d-38            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-39            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-40            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-41            [-1, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-42            [-1, 512, 4, 4]           1,024\n",
      "       BasicBlock-43            [-1, 512, 4, 4]               0\n",
      "           Conv2d-44            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-45            [-1, 512, 4, 4]           1,024\n",
      "           Conv2d-46            [-1, 512, 4, 4]       2,359,296\n",
      "      BatchNorm2d-47            [-1, 512, 4, 4]           1,024\n",
      "       BasicBlock-48            [-1, 512, 4, 4]               0\n",
      "           Linear-49                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,173,962\n",
      "Trainable params: 11,173,962\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 11.25\n",
      "Params size (MB): 42.63\n",
      "Estimated Total Size (MB): 53.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(test_net_18, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net18_model = ResNet18().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net18_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for updating learning rate\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/20, step:100/782, Loss:1.978853, Acc: 26.125000, (correct:1672/total:6400)\n",
      "Epoch:1/20, step:200/782, Loss:1.855718, Acc: 30.453125, (correct:3898/total:12800)\n",
      "Epoch:1/20, step:300/782, Loss:1.768279, Acc: 33.708333, (correct:6472/total:19200)\n",
      "Epoch:1/20, step:400/782, Loss:1.693645, Acc: 36.714844, (correct:9399/total:25600)\n",
      "Epoch:1/20, step:500/782, Loss:1.626807, Acc: 39.409375, (correct:12611/total:32000)\n",
      "Epoch:1/20, step:600/782, Loss:1.571510, Acc: 41.812500, (correct:16056/total:38400)\n",
      "Epoch:1/20, step:700/782, Loss:1.518394, Acc: 43.783482, (correct:19615/total:44800)\n",
      "Epoch:2/20, step:100/782, Loss:12.717088, Acc: 46.849291, (correct:26423/total:56400)\n",
      "Epoch:2/20, step:200/782, Loss:6.900116, Acc: 48.367834, (correct:30375/total:62800)\n",
      "Epoch:2/20, step:300/782, Loss:4.951311, Acc: 49.696532, (correct:34390/total:69200)\n",
      "Epoch:2/20, step:400/782, Loss:3.964879, Acc: 50.870370, (correct:38458/total:75600)\n",
      "Epoch:2/20, step:500/782, Loss:3.370199, Acc: 51.953659, (correct:42602/total:82000)\n",
      "Epoch:2/20, step:600/782, Loss:2.968631, Acc: 52.970588, (correct:46826/total:88400)\n",
      "Epoch:2/20, step:700/782, Loss:2.678861, Acc: 53.864979, (correct:51064/total:94800)\n",
      "Epoch:3/20, step:100/782, Loss:20.394484, Acc: 55.396617, (correct:58942/total:106400)\n",
      "Epoch:3/20, step:200/782, Loss:10.631215, Acc: 56.169326, (correct:63359/total:112800)\n",
      "Epoch:3/20, step:300/782, Loss:7.365597, Acc: 56.919463, (correct:67848/total:119200)\n",
      "Epoch:3/20, step:400/782, Loss:5.727721, Acc: 57.641720, (correct:72398/total:125600)\n",
      "Epoch:3/20, step:500/782, Loss:4.738761, Acc: 58.343939, (correct:77014/total:132000)\n",
      "Epoch:3/20, step:600/782, Loss:4.080612, Acc: 58.977601, (correct:81625/total:138400)\n",
      "Epoch:3/20, step:700/782, Loss:3.605811, Acc: 59.634669, (correct:86351/total:144800)\n",
      "Epoch:4/20, step:100/782, Loss:26.529359, Acc: 60.813299, (correct:95112/total:156400)\n",
      "Epoch:4/20, step:200/782, Loss:13.610982, Acc: 61.385749, (correct:99936/total:162800)\n",
      "Epoch:4/20, step:300/782, Loss:9.305336, Acc: 61.911348, (correct:104754/total:169200)\n",
      "Epoch:4/20, step:400/782, Loss:7.152050, Acc: 62.421412, (correct:109612/total:175600)\n",
      "Epoch:4/20, step:500/782, Loss:5.853482, Acc: 62.936264, (correct:114544/total:182000)\n",
      "Epoch:4/20, step:600/782, Loss:4.986006, Acc: 63.424098, (correct:119491/total:188400)\n",
      "Epoch:4/20, step:700/782, Loss:4.364814, Acc: 63.903491, (correct:124484/total:194800)\n",
      "Epoch:5/20, step:100/782, Loss:31.646501, Acc: 64.745640, (correct:133635/total:206400)\n",
      "Epoch:5/20, step:200/782, Loss:16.122930, Acc: 65.175282, (correct:138693/total:212800)\n",
      "Epoch:5/20, step:300/782, Loss:10.933599, Acc: 65.624544, (correct:143849/total:219200)\n",
      "Epoch:5/20, step:400/782, Loss:8.343760, Acc: 66.040780, (correct:148988/total:225600)\n",
      "Epoch:5/20, step:500/782, Loss:6.789430, Acc: 66.424569, (correct:154105/total:232000)\n",
      "Epoch:5/20, step:600/782, Loss:5.753928, Acc: 66.782718, (correct:159210/total:238400)\n",
      "Epoch:5/20, step:700/782, Loss:5.015164, Acc: 67.122141, (correct:164315/total:244800)\n",
      "Epoch:6/20, step:100/782, Loss:36.076850, Acc: 67.762090, (correct:173742/total:256400)\n",
      "Epoch:6/20, step:200/782, Loss:18.290857, Acc: 68.120624, (correct:179021/total:262800)\n",
      "Epoch:6/20, step:300/782, Loss:12.367687, Acc: 68.456909, (correct:184286/total:269200)\n",
      "Epoch:6/20, step:400/782, Loss:9.410291, Acc: 68.755806, (correct:189491/total:275600)\n",
      "Epoch:6/20, step:500/782, Loss:7.629159, Acc: 69.071277, (correct:194781/total:282000)\n",
      "Epoch:6/20, step:600/782, Loss:6.432442, Acc: 69.414355, (correct:200191/total:288400)\n",
      "Epoch:6/20, step:700/782, Loss:5.581423, Acc: 69.721506, (correct:205539/total:294800)\n",
      "Epoch:7/20, step:100/782, Loss:39.969665, Acc: 70.242820, (correct:215224/total:306400)\n",
      "Epoch:7/20, step:200/782, Loss:20.205380, Acc: 70.534527, (correct:220632/total:312800)\n",
      "Epoch:7/20, step:300/782, Loss:13.617728, Acc: 70.822055, (correct:226064/total:319200)\n",
      "Epoch:7/20, step:400/782, Loss:10.325080, Acc: 71.094287, (correct:231483/total:325600)\n",
      "Epoch:7/20, step:500/782, Loss:8.349436, Acc: 71.359337, (correct:236913/total:332000)\n",
      "Epoch:7/20, step:600/782, Loss:7.033880, Acc: 71.602541, (correct:242303/total:338400)\n",
      "Epoch:7/20, step:700/782, Loss:6.093602, Acc: 71.842517, (correct:247713/total:344800)\n",
      "Epoch:8/20, step:100/782, Loss:43.409082, Acc: 72.285073, (correct:257624/total:356400)\n",
      "Epoch:8/20, step:200/782, Loss:21.908947, Acc: 72.521499, (correct:263108/total:362800)\n",
      "Epoch:8/20, step:300/782, Loss:14.740397, Acc: 72.748646, (correct:268588/total:369200)\n",
      "Epoch:8/20, step:400/782, Loss:11.161836, Acc: 72.968051, (correct:274068/total:375600)\n",
      "Epoch:8/20, step:500/782, Loss:9.012490, Acc: 73.184555, (correct:279565/total:382000)\n",
      "Epoch:8/20, step:600/782, Loss:7.576155, Acc: 73.393924, (correct:285062/total:388400)\n",
      "Epoch:8/20, step:700/782, Loss:6.550729, Acc: 73.603597, (correct:290587/total:394800)\n",
      "Epoch:9/20, step:100/782, Loss:46.567477, Acc: 73.969242, (correct:300611/total:406400)\n",
      "Epoch:9/20, step:200/782, Loss:23.464126, Acc: 74.176841, (correct:306202/total:412800)\n",
      "Epoch:9/20, step:300/782, Loss:15.766542, Acc: 74.372376, (correct:311769/total:419200)\n",
      "Epoch:9/20, step:400/782, Loss:11.917766, Acc: 74.557801, (correct:317318/total:425600)\n",
      "Epoch:9/20, step:500/782, Loss:9.606420, Acc: 74.754861, (correct:322941/total:432000)\n",
      "Epoch:9/20, step:600/782, Loss:8.065711, Acc: 74.938869, (correct:328532/total:438400)\n",
      "Epoch:9/20, step:700/782, Loss:6.969429, Acc: 75.102743, (correct:334057/total:444800)\n",
      "Epoch:10/20, step:100/782, Loss:49.412595, Acc: 75.425285, (correct:344241/total:456400)\n",
      "Epoch:10/20, step:200/782, Loss:24.872848, Acc: 75.609334, (correct:349920/total:462800)\n",
      "Epoch:10/20, step:300/782, Loss:16.689661, Acc: 75.786019, (correct:355588/total:469200)\n",
      "Epoch:10/20, step:400/782, Loss:12.604191, Acc: 75.945963, (correct:361199/total:475600)\n",
      "Epoch:10/20, step:500/782, Loss:10.150655, Acc: 76.110166, (correct:366851/total:482000)\n",
      "Epoch:10/20, step:600/782, Loss:8.519491, Acc: 76.263718, (correct:372472/total:488400)\n",
      "Epoch:10/20, step:700/782, Loss:7.352667, Acc: 76.418957, (correct:378121/total:494800)\n",
      "Epoch:11/20, step:100/782, Loss:52.045115, Acc: 76.710703, (correct:388463/total:506400)\n",
      "Epoch:11/20, step:200/782, Loss:26.180413, Acc: 76.868175, (correct:394180/total:512800)\n",
      "Epoch:11/20, step:300/782, Loss:17.551890, Acc: 77.030431, (correct:399942/total:519200)\n",
      "Epoch:11/20, step:400/782, Loss:13.242270, Acc: 77.174087, (correct:405627/total:525600)\n",
      "Epoch:11/20, step:500/782, Loss:10.655485, Acc: 77.315789, (correct:411320/total:532000)\n",
      "Epoch:11/20, step:600/782, Loss:8.932615, Acc: 77.454123, (correct:417013/total:538400)\n",
      "Epoch:11/20, step:700/782, Loss:7.701520, Acc: 77.589758, (correct:422709/total:544800)\n",
      "Epoch:12/20, step:100/782, Loss:54.461563, Acc: 77.839684, (correct:433100/total:556400)\n",
      "Epoch:12/20, step:200/782, Loss:27.377216, Acc: 77.976724, (correct:438853/total:562800)\n",
      "Epoch:12/20, step:300/782, Loss:18.341947, Acc: 78.119466, (correct:444656/total:569200)\n",
      "Epoch:12/20, step:400/782, Loss:13.828052, Acc: 78.252606, (correct:450422/total:575600)\n",
      "Epoch:12/20, step:500/782, Loss:11.118384, Acc: 78.382646, (correct:456187/total:582000)\n",
      "Epoch:12/20, step:600/782, Loss:9.310720, Acc: 78.514616, (correct:461980/total:588400)\n",
      "Epoch:12/20, step:700/782, Loss:8.021160, Acc: 78.638366, (correct:467741/total:594800)\n",
      "Epoch:13/20, step:100/782, Loss:56.640643, Acc: 78.865270, (correct:478239/total:606400)\n",
      "Epoch:13/20, step:200/782, Loss:28.450599, Acc: 78.991678, (correct:484061/total:612800)\n",
      "Epoch:13/20, step:300/782, Loss:19.051631, Acc: 79.114987, (correct:489880/total:619200)\n",
      "Epoch:13/20, step:400/782, Loss:14.354404, Acc: 79.236893, (correct:495706/total:625600)\n",
      "Epoch:13/20, step:500/782, Loss:11.535423, Acc: 79.355854, (correct:501529/total:632000)\n",
      "Epoch:13/20, step:600/782, Loss:9.656468, Acc: 79.469612, (correct:507334/total:638400)\n",
      "Epoch:13/20, step:700/782, Loss:8.313712, Acc: 79.578784, (correct:513124/total:644800)\n",
      "Epoch:14/20, step:100/782, Loss:58.660582, Acc: 79.780774, (correct:523681/total:656400)\n",
      "Epoch:14/20, step:200/782, Loss:29.448005, Acc: 79.893935, (correct:529537/total:662800)\n",
      "Epoch:14/20, step:300/782, Loss:19.712035, Acc: 80.007472, (correct:535410/total:669200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:14/20, step:400/782, Loss:14.846225, Acc: 80.115897, (correct:541263/total:675600)\n",
      "Epoch:14/20, step:500/782, Loss:11.925484, Acc: 80.225220, (correct:547136/total:682000)\n",
      "Epoch:14/20, step:600/782, Loss:9.978239, Acc: 80.333236, (correct:553014/total:688400)\n",
      "Epoch:14/20, step:700/782, Loss:8.586854, Acc: 80.435809, (correct:558868/total:694800)\n",
      "Epoch:15/20, step:100/782, Loss:60.538881, Acc: 80.624292, (correct:569530/total:706400)\n",
      "Epoch:15/20, step:200/782, Loss:30.373648, Acc: 80.731762, (correct:575456/total:712800)\n",
      "Epoch:15/20, step:300/782, Loss:20.325100, Acc: 80.831479, (correct:581340/total:719200)\n",
      "Epoch:15/20, step:400/782, Loss:15.297526, Acc: 80.930402, (correct:587231/total:725600)\n",
      "Epoch:15/20, step:500/782, Loss:12.282547, Acc: 81.028689, (correct:593130/total:732000)\n",
      "Epoch:15/20, step:600/782, Loss:10.272424, Acc: 81.122291, (correct:599007/total:738400)\n",
      "Epoch:15/20, step:700/782, Loss:8.836927, Acc: 81.218448, (correct:604915/total:744800)\n",
      "Epoch:16/20, step:100/782, Loss:62.254729, Acc: 81.390799, (correct:615640/total:756400)\n",
      "Epoch:16/20, step:200/782, Loss:31.232282, Acc: 81.485711, (correct:621573/total:762800)\n",
      "Epoch:16/20, step:300/782, Loss:20.890125, Acc: 81.579563, (correct:627510/total:769200)\n",
      "Epoch:16/20, step:400/782, Loss:15.720308, Acc: 81.671867, (correct:633447/total:775600)\n",
      "Epoch:16/20, step:500/782, Loss:12.617125, Acc: 81.762276, (correct:639381/total:782000)\n",
      "Epoch:16/20, step:600/782, Loss:10.548198, Acc: 81.853120, (correct:645330/total:788400)\n",
      "Epoch:16/20, step:700/782, Loss:9.071481, Acc: 81.940614, (correct:651264/total:794800)\n",
      "Epoch:17/20, step:100/782, Loss:63.872659, Acc: 82.095734, (correct:662020/total:806400)\n",
      "Epoch:17/20, step:200/782, Loss:32.021351, Acc: 82.191929, (correct:668056/total:812800)\n",
      "Epoch:17/20, step:300/782, Loss:21.402871, Acc: 82.283691, (correct:674068/total:819200)\n",
      "Epoch:17/20, step:400/782, Loss:16.100517, Acc: 82.368702, (correct:680036/total:825600)\n",
      "Epoch:17/20, step:500/782, Loss:12.924718, Acc: 82.446034, (correct:685951/total:832000)\n",
      "Epoch:17/20, step:600/782, Loss:10.802792, Acc: 82.529819, (correct:691930/total:838400)\n",
      "Epoch:17/20, step:700/782, Loss:9.289166, Acc: 82.606416, (correct:697859/total:844800)\n",
      "Epoch:18/20, step:100/782, Loss:65.350834, Acc: 82.756889, (correct:708730/total:856400)\n",
      "Epoch:18/20, step:200/782, Loss:32.764581, Acc: 82.837854, (correct:714725/total:862800)\n",
      "Epoch:18/20, step:300/782, Loss:21.901908, Acc: 82.920156, (correct:720742/total:869200)\n",
      "Epoch:18/20, step:400/782, Loss:16.471322, Acc: 82.997031, (correct:726722/total:875600)\n",
      "Epoch:18/20, step:500/782, Loss:13.210223, Acc: 83.077664, (correct:732745/total:882000)\n",
      "Epoch:18/20, step:600/782, Loss:11.038163, Acc: 83.154998, (correct:738749/total:888400)\n",
      "Epoch:18/20, step:700/782, Loss:9.488618, Acc: 83.230554, (correct:744747/total:894800)\n",
      "Epoch:19/20, step:100/782, Loss:66.728164, Acc: 83.369153, (correct:755658/total:906400)\n",
      "Epoch:19/20, step:200/782, Loss:33.441299, Acc: 83.449167, (correct:761724/total:912800)\n",
      "Epoch:19/20, step:300/782, Loss:22.351163, Acc: 83.522846, (correct:767742/total:919200)\n",
      "Epoch:19/20, step:400/782, Loss:16.803200, Acc: 83.597774, (correct:773781/total:925600)\n",
      "Epoch:19/20, step:500/782, Loss:13.476087, Acc: 83.668884, (correct:779794/total:932000)\n",
      "Epoch:19/20, step:600/782, Loss:11.256550, Acc: 83.740622, (correct:785822/total:938400)\n",
      "Epoch:19/20, step:700/782, Loss:9.671412, Acc: 83.812659, (correct:791862/total:944800)\n",
      "Epoch:20/20, step:100/782, Loss:67.964640, Acc: 83.947616, (correct:802875/total:956400)\n",
      "Epoch:20/20, step:200/782, Loss:34.057659, Acc: 84.017761, (correct:808923/total:962800)\n",
      "Epoch:20/20, step:300/782, Loss:22.756348, Acc: 84.086257, (correct:814964/total:969200)\n",
      "Epoch:20/20, step:400/782, Loss:17.106435, Acc: 84.157134, (correct:821037/total:975600)\n",
      "Epoch:20/20, step:500/782, Loss:13.714564, Acc: 84.226884, (correct:827108/total:982000)\n",
      "Epoch:20/20, step:600/782, Loss:11.456538, Acc: 84.289660, (correct:833119/total:988400)\n",
      "Epoch:20/20, step:700/782, Loss:9.843305, Acc: 84.353036, (correct:839144/total:994800)\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "pre_epoch_total_step = len(trainloader)\n",
    "current_lr = learning_rate\n",
    "net18_model.train()\n",
    "train_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, (x, y) in enumerate(trainloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # forward\n",
    "        prediction = net18_model(x)\n",
    "        loss = criterion(prediction, y)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = prediction.max(1)\n",
    "        total += y.size(0)\n",
    "        correct += predicted.eq(y).sum().item()\n",
    "        \n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            template = r\"Epoch:{}/{}, step:{}/{}, Loss:{:.6f}, Acc: {:.6f}, (correct:{}/total:{})\"\n",
    "            print(template.format(epoch+1,\n",
    "                                  EPOCHS, \n",
    "                                  i+1, \n",
    "                                  pre_epoch_total_step, \n",
    "                                  train_loss/(i+1),\n",
    "                                  100.*correct/total,\n",
    "                                 correct,\n",
    "                                 total))\n",
    "\n",
    "    # decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        current_lr = current_lr/2\n",
    "        update_lr(optimizer, current_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:90.38%\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "net18_model.eval()\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for x, y in testloader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        prediction = net18_model(x)\n",
    "        _, predic = torch.max(prediction.data, dim=1)\n",
    "        total += y.size(0)\n",
    "        correct += (predic == y).sum().item()\n",
    "\n",
    "    print(\"Accuracy:{}%\".format(100 * correct / total))\n",
    "\n",
    "model_path = r'./cifar10_data/model/'\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "model_name = os.path.join(model_path, 'cifar10_resnet.ckpt')\n",
    "# save model\n",
    "torch.save(net18_model.state_dict(), model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

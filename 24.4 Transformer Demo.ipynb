{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打算详细深入了解bert和transform的原理和源代码，之前已经读完了“基于attention的seq2seq机器翻译”源代码，这篇是Transformer，代码来自tf2.0官网demo，个人修正了一部分bug，可以跑通，并做了更详尽的注释。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 备注"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "西班牙语-英语的翻译，自己换个训练文件也能跑通\n",
    "\n",
    "参考自官方demo，网址https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "\n",
    "原代码有部分bug，已修正，保证跑通\n",
    "\n",
    "注释更详尽，已尽力做到傻瓜式\n",
    "\n",
    "官网有更多的结构图和数学理论，看不懂的可以结合官网理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"   \n",
    "# 本地调试我用的核显本，无cuda，故屏蔽GPU，实际上不加这么一句也无所谓，因为检测不到GPU的话就自动调用cpu了，主要还是为了调试方便，比如有时候你显存不够用，就只能先屏蔽GPU了\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import io\n",
    "import time\n",
    "# if gpus:\n",
    "#     try:    \n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     except RuntimeError as e:\n",
    "#         print(e)\n",
    "# #设置增长式显存占用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', \n",
    "                               with_info=True,\n",
    "                               as_supervised=True)\n",
    "#api手册里，这个tfds.load讲了一大堆，实际业务中估计也不会用这个函数提供的数据，有兴趣的可以看看手册\n",
    "#从手册里找到这么一句：Loads the named dataset into a `tf.data.Dataset`，\n",
    "#就是将指定的数据集下载下来(DataSet格式)，大概理解就行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples, val_examples = examples['train'], examples['validation']\n",
    "#这个train_examples是个PrefetchDataset，但是之前的tfds.load函数手册里并没有提到这点，不知为何\n",
    "#虽然并不影响阅读代码，但是不用迭代器看不了数据，显示为None\n",
    "#关于PrefetchDataset的解释，后面有，现在把他当普通Dataset理解就行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((), ()), types: (tf.string, tf.string)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = \\\n",
    "tfds.deprecated.text.SubwordTextEncoder.build_from_corpus((en.numpy() \n",
    "                                                           for pt, en \n",
    "                                                           in train_examples), \n",
    "                                                          target_vocab_size=2**13)\n",
    "#原文是tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus((en.numpy() for pt, en in train_examples), target_vocab_size=2**13)\n",
    "#但是总提示features下没有text这个方法，后来查tf官网，才知道text方法已经被弃用，挪到deprecated下了\n",
    "#功能就是把文本变成编码，同时也提供解码功能，下面的几行示例很好的展示了作用\n",
    "#类似功能的函数很多，keras下也有替代品，根据情况自己写也行；无需理解，反正也被弃用了\n",
    "tokenizer_pt = \\\n",
    "tfds.deprecated.text.SubwordTextEncoder.build_from_corpus((pt.numpy() \n",
    "                                                           for pt, en \n",
    "                                                           in train_examples), \n",
    "                                                          target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [7915, 1248, 7946, 7194, 13, 2799, 7877]\n",
      "The original string: Transformer is awesome.\n"
     ]
    }
   ],
   "source": [
    "sample_string = 'Transformer is awesome.'\n",
    "\n",
    "tokenized_string = tokenizer_en.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "#输出：Tokenized string is [7915, 1248, 7946, 7194, 13, 2799, 7877]\n",
    "#很明显，编码后维度超出了原文长度，因为如果单词不在词典中，\n",
    "#分词器会把单词分解为子词来对字符串进行编码\n",
    "#后面会有示例\n",
    "\n",
    "original_string = tokenizer_en.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n",
    "# 输出：The original string: Transformer is awesome.\n",
    "\n",
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7915 ----> T\n",
      "1248 ----> ran\n",
      "7946 ----> s\n",
      "7194 ----> former \n",
      "13 ----> is \n",
      "2799 ----> awesome\n",
      "7877 ----> .\n"
     ]
    }
   ],
   "source": [
    "for ts in tokenized_string:\n",
    " print('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))\n",
    "# 输出：\n",
    "#7915 ----> T\n",
    "#1248 ----> ran\n",
    "#7946 ----> s\n",
    "#7194 ----> former \n",
    "#13 ----> is \n",
    "#2799 ----> awesome\n",
    "#7877 ----> ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一些参数\n",
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "MAX_LENGTH = 40  #为了使本示例较小且相对较快，删除长度大于40个标记的样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(lang1, lang2):\n",
    "    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "    # [葡萄牙语词汇量]+lang1所有词汇编码后的列表+[葡萄牙语词汇量+1]，这三个列表合并\n",
    "    # 本例中，葡萄牙语词汇量是8214，后续处理文本的时候，8214当做起始符，8215当做终止符\n",
    "    # 实际业务中按照自己的习惯处理就行，比如：\n",
    "    # 在句子首位分别加上<start>和<end>，然后整个语料库一起扔进tf.keras.preprocessing.text.Tokenizer模块里编码\n",
    "\n",
    "    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "    # 英语，处理逻辑同上\n",
    "  \n",
    "    return lang1, lang2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,tf.size(y) <= max_length)\n",
    "# 这个tf.logical_and就是 逻辑与 的功能\n",
    "# 因为训练的时候用了个tf.function装饰器，tf就使用静态图计算\n",
    "# 我在https://blog.csdn.net/ziyi9663/article/details/109989814这篇文章里也提到过\n",
    "# 这样做可以加速计算，但是对tensor值的一些操作就不能直接进行了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于这个tf_encode()函数中tf.py_function()方法，源代码的注释是： .map() 内部的操作以图模式（graph mode）运行，.map() 接收一个不具有 numpy 属性的图张量（graph tensor）。该分词器（tokenizer）需要将一个字符串或 Unicode 符号，编码成整数。因此，您需要在 tf.py_function 内部运行编码过程，tf.py_function 接收一个 eager 张量，该 eager 张量有一个包含字符串值的 numpy 属性。\n",
    "\n",
    "上面这段说得有点复杂，重新查了下tf.py_function的api手册，api手册是这么说的：Wraps a python function into a TensorFlow op that executes it eagerly.说人话，还是关于静态图和动态图的那一套，为了计算速度用了静态图，中途要对张量做处理，就得用点别的办法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_encode(pt, en):\n",
    "    result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n",
    "    # 用tf.py_function包裹encode函数\n",
    "    \n",
    "    result_pt.set_shape([None])\n",
    "    # 句子维度设置为None，避免定长带来问题，不理解的话可以先跳过set shape这个方法往下看，不影响理解代码\n",
    "    result_en.set_shape([None])\n",
    "    \n",
    "    return result_pt, result_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_examples.map(tf_encode)\n",
    "#这里的map，对train_examples的每个元素，使用tf_encode方法\n",
    "#train_examples一个元素就是一个葡萄牙语-英语句子对，两个变量，对应tf_encode两个参数\n",
    "#然后这俩参数传给被tf_encode包裹的encode函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.filter(filter_max_length)\n",
    "#dataset的filter方法，感觉就是Python的filter函数的精简版，官方doc太长不看：\n",
    "#就是传个函数，这个函数对数据集的每条数据做处理，返回True的数据就保留\n",
    "#本例中，就是葡萄牙语句子和英语句子长度都不大于MAX_LENGTH(就是40)的训练数据(一条数据是葡语和英语句子对)才得以保留\n",
    "#有兴趣的可以查看官方手册，但其实filter方法就这么一个参数，也没啥高级功能可看的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.cache()\n",
    "#将数据集缓存到内存中以加快读取速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
    "#这个padded_batch就是结合了padding和batch的操作，有个padded_shapes参数(这里没传参)，默认[-1]，就是按照最大长度进行padding\n",
    "#前面虽然用filter函数筛选了，但是长度不足的还得补0\n",
    "#刚开始还在想为什么不跳过filter步骤，直接在这里传个[40]进去，然后很快就想明白了，这是翻译，把超出长度的句子砍个尾巴，那翻译出来肯定有问题\n",
    "#文本分类有时候可能不在意，可以尝试直接padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "#关于prefetch方法：\n",
    "#提前加载数据并用cpu处理，避免传统流程：cpu处理一个batch的数据——GPU计算——cpu处理一个batch的数据——GPU计算。。。如此循环\n",
    "#避免GPU在等cpu，加快速度\n",
    "#预加载量可以自己设定，一般大于batch size才有意义，\n",
    "#这里设置的是tf.data.experimental.AUTOTUNE，就是让tf根据gpu计算速度动态调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_max_length).padded_batch(BATCH_SIZE)\n",
    "#验证集，一个处理方法，也没必要用prefetch方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[8214  342 3032 ...    0    0    0]\n",
      " [8214   95  198 ...    0    0    0]\n",
      " [8214 4479 7990 ...    0    0    0]\n",
      " ...\n",
      " [8214  584   12 ...    0    0    0]\n",
      " [8214   59 1548 ...    0    0    0]\n",
      " [8214  118   34 ...    0    0    0]], shape=(64, 38), dtype=int64) tf.Tensor(\n",
      "[[8087   98   25 ...    0    0    0]\n",
      " [8087   12   20 ...    0    0    0]\n",
      " [8087   12 5453 ...    0    0    0]\n",
      " ...\n",
      " [8087   18 2059 ...    0    0    0]\n",
      " [8087   16 1436 ...    0    0    0]\n",
      " [8087   15   57 ...    0    0    0]], shape=(64, 40), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "pt_batch, en_batch = next(iter(val_dataset))\n",
    "# 一开始就说过，不用迭代器看不了数据\n",
    "print(pt_batch, en_batch)\n",
    "# 输出：\n",
    "# tf.Tensor(\n",
    "# [[8214  342 3032 ...    0    0    0]\n",
    "#  [8214   95  198 ...    0    0    0]\n",
    "#  [8214 4479 7990 ...    0    0    0]\n",
    "#  ...\n",
    "#  [8214  584   12 ...    0    0    0]\n",
    "#  [8214   59 1548 ...    0    0    0]\n",
    "#  [8214  118   34 ...    0    0    0]], shape=(64, 38), dtype=int64) tf.Tensor(\n",
    "# [[8087   98   25 ...    0    0    0]\n",
    "#  [8087   12   20 ...    0    0    0]\n",
    "#  [8087   12 5453 ...    0    0    0]\n",
    "#  ...\n",
    "#  [8087   18 2059 ...    0    0    0]\n",
    "#  [8087   16 1436 ...    0    0    0]\n",
    "#  [8087   15   57 ...    0    0    0]], shape=(64, 40), dtype=int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 位置编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为该模型并不包括任何的循环（recurrence）或卷积，所以模型添加了位置编码，为模型提供一些关于单词在句子中相对位置的信息。\n",
    "\n",
    "位置编码向量被加到嵌入（embedding）向量中。嵌入表示一个 d 维空间的标记，在 d 维空间中有着相似含义的标记会离彼此更近。但是，嵌入并没有对在一句话中的词的相对位置进行编码。因此，当加上位置编码后，词将基于它们含义的相似度以及它们在句子中的位置，在 d 维空间中离彼此更近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    \"\"\"就是计算公式里sin和cos括号里的，参数类型是我根据下面的代码推测的，方便理解本函数\n",
    "    Args:\n",
    "        pos: ndarray型\n",
    "        i: 也是ndarray型\n",
    "        d_model: int型\n",
    "    Returns:\n",
    "        计算公式里sin和cos括号里的部分\n",
    "    \"\"\"\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    # 以后面的调用为例，传入的pos是维度(50,1)的ndarray，i是维度(1,512)的ndarray，d_model是整型512\n",
    "    # 于是angle_rates也是(1,512)的ndarray\n",
    "     \n",
    "    return pos * angle_rates\n",
    "    # 于是返回值pos * angle_rates的维度就是(50,512)\n",
    "    # 从这里也可以看到，位置编码是根据position的值一下子把整个序列的每一时间步的编码都算出来的\n",
    "    # 实际用的时候，根据序列长度，对位置编码切片取值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于下面函数所用的部分方法备注：\n",
    "\n",
    "np.arange()函数和Python自带的range()功能类似，在本例中，可认为功能一致，但是返回的是ndarray而非列表；\n",
    "np.newaxis并不是在原地生成什么，直白地说，就是对一个ndarray用[:, np.newaxis]，就是在axis=1处生成一个新维度，例如：\n",
    "np.arange(5)结果是array([0, 1, 2, 3, 4])，维度是(5,)，np.arange(5)[:, np.newaxis]结果是array([[0],[1],[2],[3],[4]])，维度就是(5,1)；\n",
    "np.arange(5)[np.newaxis,:]结果是array([[0, 1, 2, 3, 4]])，维度是(1,5)；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # 将 sin 应用于数组中的偶数索引（indices）；2i\n",
    "\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    # 将 cos 应用于数组中的奇数索引；2i+1\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    # 这个省略号愣了好几秒，才想起了Python教材里关于列表的好些切片知识\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    # 最后的维度是(1,position,d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单测试下positional_encoding函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding.shape)\n",
    "#输出(1, 50, 512)\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 遮挡（Masking）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遮挡一批序列中所有的填充标记（pad tokens）。\n",
    "\n",
    "这确保了模型不会将填充作为输入。\n",
    "\n",
    "该 mask 表明填充值 0 出现的位置：在这些位置 mask 输出 1，否则输出 0。\n",
    "\n",
    "这个是官网的注释，看着就语句不顺，不看也行，代码部分有更详细的注释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    \"\"\"创建padding mask，功能上来说，就是对于把序列中不是0的换成0，0换成1，1就表示被遮挡了\n",
    "    Args:\n",
    "        seq:输入的数据，维度是(batch_size,seq_len)\n",
    "    Returns:\n",
    "        处理后的结果，顺便增加了维度，变成(batch_size, 1, 1, seq_len)\n",
    "    \"\"\"\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "    # 这个tf.newaxis和np.newaxis效果一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create_padding_mask的调用示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 1. 1. 0.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[1. 1. 1. 0. 0.]]]], shape=(3, 1, 1, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "print(create_padding_mask(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前瞻遮挡（look-ahead mask）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原注释：用于遮挡一个序列中的后续标记（future tokens）。换句话说，该 mask 表明了不应该使用的条目。\n",
    "\n",
    "这意味着要预测第三个词，将仅使用第一个和第二个词。与此类似，预测第四个词，仅使用第一个，第二个和第三个词，依此类推。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "关于tf.linalg.band_part函数：\n",
    "这是个关于三角阵操作的函数，函数api手册说得挺花哨的，功能就是保留矩阵的非主对角线元素，其余的换成0\n",
    "接收三个参数：\n",
    "input: 输入的张量;\n",
    "num_lower ：指定保留的主对角线下方的副对角线的数量，输入数值为负数时，\n",
    "表示下方的对角矩阵元素全部保留；\n",
    "num_upper：指定保留的主对角线上方的副对角线的数量，输入数值为负数时，\n",
    "表示上方的对角矩阵元素全部保留；\n",
    "这样解释就很清晰明了了，自己跑一跑就更清楚了\n",
    "\"\"\"\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    # 先产生个幺矩阵，再把主对角线上方的元素置零，变成了下三角阵，\n",
    "    # 1减这个下三角阵，又变成了上三角阵（此时主对角线也全是0）\n",
    "    # 在模型中的作用，后续会有解释，目前先记住代码功能就行\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 0.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "print(temp)\n",
    "# 结果是：\n",
    "# tf.Tensor(\n",
    "# [[0. 1. 1.]\n",
    "#  [0. 0. 1.]\n",
    "#  [0. 0. 0.]], shape=(3, 3), dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 按比缩放的点积注意力（Scaled dot product attention）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原注释：Transformer 使用的注意力函数有三个输入：\n",
    "\n",
    "Q（请求（query））、K（主键（key））、V（数值（value））。\n",
    "\n",
    "用于计算注意力权重的等式为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "点积注意力被缩小了深度的平方根倍。这样做是因为对于较大的深度值，点积的大小会增大，从而推动 softmax 函数往仅有很小的梯度的方向靠拢，导致了一种很硬的（hard）softmax。\n",
    "\n",
    "例如，假设 Q 和 K 的均值为0，方差为1。它们的矩阵乘积将有均值为0，方差为 dk。\n",
    "\n",
    "因此，dk 的平方根被用于缩放（而非其他数值），因为，Q 和 K 的矩阵乘积的均值本应该为 0，方差本应该为1，这样会获得一个更平缓的 softmax。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"计算注意力权重。\n",
    "    q, k, v 必须具有匹配的前置维度\n",
    "    k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v\n",
    "    虽然 mask 根据其类型（填充或前瞻）有不同的形状\n",
    "    但是 mask 必须能进行广播转换以便求和\n",
    "\n",
    "    参数:\n",
    "        q: 请求的形状 == (..., seq_len_q, depth)\n",
    "        k: 主键的形状 == (..., seq_len_k, depth)\n",
    "        v: 数值的形状 == (..., seq_len_v, depth_v)\n",
    "        mask: Float 张量，其形状能转换成\n",
    "              (..., seq_len_q, seq_len_k)。默认为None。\n",
    "\n",
    "    返回值:\n",
    "        output:输出\n",
    "        attention_weights:注意力权重，输出权重本质上没什么用，但是可以用来可视化注意力图\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  \n",
    "    # tf.matmul，就是矩阵乘法，会自动处理好batch等维度\n",
    "    # transpose_b=True就是先把右乘的矩阵转置再计算\n",
    "    # 最后得到的矩阵维度就是(..., seq_len_q, seq_len_k)\n",
    "    # 这一步就是计算公式里的q，k内积，不清楚的可以去官方页面查看公式\n",
    "    # q，k，v什么含义一定要先看明白\n",
    "\n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    # 用tf.shape获取的张量维度是int型的，取最后一个维度数值再转float\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    # 这一步就是q、k内积再除以dk^0.5\n",
    "\n",
    "    if mask is not None:scaled_attention_logits += (mask * -1e9) \n",
    "    # 将 mask 加入到缩放的张量上 \n",
    "    # 原文没对这个做说明，我的理解是，需要屏蔽的位置(比如序列长度不足填充0的地方)，mask矩阵对应的位置就是1(可以看一下前面创建mask的两个函数)\n",
    "    # 1*-1e9，那就变成很小的数字，经softmax函数后，分到的attention就很小\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "    # softmax 在最后一个轴（seq_len_k）上归一化，因此分数相加等于1\n",
    "    # 前面的维度可以不看，最后二维就是个矩阵\n",
    "    # seq_len_q行seq_len_k列，一行数据就是k的各个时间步的权重\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "    # 结合q、k、v的知识，seq_len_v是等于seq_len_k的，所以这里可以乘\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[8.4332744e-26 1.0000000e+00 8.4332744e-26 8.4332744e-26]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[1.000000e+01 9.276602e-25]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 用于测试下qkv计算的函数\n",
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(q, k, v, None)\n",
    "    print ('Attention weights are:')\n",
    "    print (temp_attn)\n",
    "    print ('Output is:')\n",
    "    print (temp_out)\n",
    "    np.set_printoptions(suppress=True)\n",
    "# 设置supress=True意味着打印出来的小数不用科学记数法\n",
    "\n",
    "# 随机给个q、k、v瞧瞧：\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "# 实际上这里是给了query一个时间步的状态向量，所以第一个维度是1，并不影响q和k的矩阵乘法，应该都懂\n",
    "print_out(temp_q, temp_k, temp_v)  \n",
    "# 输出：\n",
    "# Attention weights are:\n",
    "# tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
    "# Output is:\n",
    "# tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 原文注释说了一堆，我觉得没必要看，就是换个q瞧瞧\n",
    "# 不放心的可以去看看原文注释\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)\n",
    "# 输出：\n",
    "# Attention weights are:\n",
    "# tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
    "# Output is:\n",
    "# tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 将所有请求一起传递\n",
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多头注意力（Multi-head attention）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原注释：\n",
    "多头注意力由四部分组成：\n",
    "\n",
    "线性层并分拆成多头。\n",
    "按比缩放的点积注意力。\n",
    "多头及联。\n",
    "最后一层线性层。\n",
    "\n",
    "每个多头注意力块有三个输入：Q（请求）、K（主键）、V（数值）。这些输入经过线性（Dense）层，并分拆成多头。\n",
    "\n",
    "将上面定义的 scaled_dot_product_attention 函数应用于每个头（进行了广播（broadcasted）以提高效率）。注意力这步必须使用一个恰当的 mask。然后将每个头的注意力输出连接起来（用tf.transpose 和 tf.reshape），并放入最后的 Dense 层。\n",
    "\n",
    "Q、K、和 V 被拆分到了多个头，而非单个的注意力头，因为多头允许模型共同注意来自不同表示空间的不同位置的信息。在分拆后，每个头部的维度减少，因此总的计算成本与有着全部维度的单个注意力头相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        '''\n",
    "        description: 初始化类\n",
    "        param {d_model:从后面的encoder部分代码看，d_model是指输入数据单个时间步的维度，或者输入数据的深度，反正就那么个意思，个人称呼不一样；\n",
    "               num_heads:多头注意力的头数}\n",
    "        return {None}\n",
    "        '''\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "        # 确保能够平均拆分到每个attention head上\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "        # 刚开始我在想，有了上面的assert，这里取整除有何意义\n",
    "        # 动手写了写，发现意义在于，这样得到的结果就是整数了，而直接用除法得到的是float\n",
    "        # 大概这是取得int型结果的多种写法吧。。。\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        # 虽然用了全连接层，但是并没有改变数据单个时间步维度\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "    def split_heads(self, x, batch_size):\n",
    "        '''\n",
    "        description: 输入数据拆分成多头\n",
    "        param {x:就是输入数据，维度是(batch_size, seq_len, d_model)；\n",
    "               batch_size:就是batch_size}\n",
    "        return {经过拆分后的数据，维度是(batch_size, num_heads, seq_len, depth)}\n",
    "        '''\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        # 别被这个depth参数名误导了，刚开始直接把他当做单个时间步状态的维度，就一直在质疑代码的正确性\n",
    "        # 实际上这个depth是每个attention head的深度，在__init__里有计算过程，一不小心就忘了\n",
    "        # 参数-1表示那个维度自动计算\n",
    "        # 最后的维度就是(batch_size, seq_len, num_heads, depth)\n",
    "\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        # 关于这个tf.transpose函数：\n",
    "        # perm参数控制转置的操作，例如transpose一个维度是2×3×4的三维张量，perm = [0,1,2]，0代表的是第一个维度，1代表第二个维度，2代表第三个维度，这样得到的就是原来的值\n",
    "        # 现在令perm=[1,0,2]，表示第一个维度和第二个维度转置，得到的张量维度就是3×2×4\n",
    "        # 有兴趣的可以看官方api手册\n",
    "\n",
    "        # 于是，最终返回的数据维度就是(batch_size, num_heads, seq_len, depth)\n",
    "    def call(self, v, k, q, mask):\n",
    "        '''\n",
    "        description: 命名为call的方法能调用父类的__call__魔法函数，反正就是tf规定子类用于计算的函数必须命名为call，就像torch必须命名为forward一样\n",
    "        param {v:value；\n",
    "               k:key;\n",
    "               q:query;\n",
    "               mask:mask张量}\n",
    "        return {output:输出，就是经注意力加权后的张量;\n",
    "                attention_weights:注意力权重}\n",
    "        '''\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        # 结合前面的scaled_dot_product_attention函数定义：\n",
    "        # scaled_attention维度是(batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights维度是(batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "        # reshape原理是先打平再改维度，所以这里把多头注意力拼接的操作，需要先transpose，再reshape\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个 MultiHeadAttention 层进行尝试。在序列中的每个位置 y，MultiHeadAttention 在序列中的所有其他位置运行所有8个注意力头，在每个位置y，返回一个新的同样长度的向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 60, 512) (1, 8, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "print(out.shape, attn.shape)\n",
    "# 输出：(1, 60, 512) (1, 8, 60, 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 点式前馈网络（Point wise feed forward network）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "点式前馈网络由两层全联接层组成，两层之间有一个 ReLU 激活函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个太简单了，没啥好注释的\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    # 记得这里有个dff参数，调用时都得传参，免得看到后面一头雾水还得回来翻代码\n",
    "    return tf.keras.Sequential([\n",
    "                                tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "                                tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "print(sample_ffn(tf.random.uniform((64, 50, 512))).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编码与解码（Encoder and decoder）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型与标准的具有注意力机制的序列到序列模型（sequence to sequence with attention model），遵循相同的一般模式。\n",
    "\n",
    "输入语句经过 N 个编码器层，为序列中的每个词/标记生成一个输出。\n",
    "解码器关注编码器的输出以及它自身的输入（自注意力）来预测下一个词。\n",
    "\n",
    "编码器层（Encoder layer）\n",
    "每个编码器层包括以下子层：\n",
    "\n",
    "多头注意力（有填充遮挡）\n",
    "点式前馈网络（Point wise feed forward networks）。\n",
    "每个子层在其周围有一个残差连接，然后进行层归一化。残差连接有助于避免深度网络中的梯度消失问题。\n",
    "\n",
    "每个子层的输出是 LayerNorm(x + Sublayer(x))。归一化是在 d_model（最后一个）维度完成的。Transformer 中有 N 个编码器层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个注释得比较少，没啥好注释的\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        # 训练时training参数设为True，dropout生效\n",
    "        # 计算时设为False，dropout就不生效了\n",
    "\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        # 残差\n",
    "        # 其实我觉得这里虽然和残差结构一样，但并不是残差的意义，\n",
    "        #而是原序列的特征向量q，加上注意力向量v\n",
    "        # 不然难道只用一个注意力向量v？那也不合理\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 43, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer(tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "print(sample_encoder_layer_output.shape)  # (batch_size, input_seq_len, d_model)\n",
    "# 输出：(64, 43, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解码器层（Decoder layer）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个解码器层包括以下子层：\n",
    "\n",
    "遮挡的多头注意力（前瞻遮挡和填充遮挡）\n",
    "多头注意力（用填充遮挡）。V（数值）和 K（主键）接收编码器输出作为输入。Q（请求）接收遮挡的多头注意力子层的输出。\n",
    "点式前馈网络\n",
    "每个子层在其周围有一个残差连接，然后进行层归一化。每个子层的输出是 LayerNorm(x + Sublayer(x))。归一化是在 d_model（最后一个）维度完成的。\n",
    "\n",
    "Transformer 中共有 N 个解码器层。\n",
    "\n",
    "当 Q 接收到解码器的第一个注意力块的输出，并且 K 接收到编码器的输出时，注意力权重表示根据编码器的输出赋予解码器输入的重要性。换一种说法，解码器通过查看编码器输出和对其自身输出的自注意力，预测下一个词。参看按比缩放的点积注意力部分的演示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本上一目了然\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        # 一个DecoderLayer有两个注意力，一个是真实数据的注意力\n",
    "        # 一个是前一个注意力的输出相对编码器那边输出状态的注意力\n",
    "        # 看一下网络结构图就能理解\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "    False, None, None)\n",
    "\n",
    "print(sample_decoder_layer_output.shape ) # (batch_size, target_seq_len, d_model)\n",
    "# 输出：TensorShape([64, 50, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 编码器（Encoder）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编码器 包括： 1. 输入嵌入（Input Embedding） 2. 位置编码（Positional Encoding） 3. N 个编码器层（encoder layers）\n",
    "\n",
    "输入经过嵌入（embedding）后，该嵌入与位置编码相加。该加法结果的输出是编码器层的输入。编码器的输出是解码器的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 没太多可注释的，so easy\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        # 对第二个维度，也就是位置切片（不清楚原因的话回头翻翻positional_encoding函数及其调用示例）\n",
    "        # 然后将嵌入和位置编码相加\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, dff=2048, input_vocab_size=8500,maximum_position_encoding=10000)\n",
    "\n",
    "sample_encoder_output = sample_encoder(tf.random.uniform((64, 62)), \n",
    "                                       training=False, mask=None)\n",
    "\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解码器（Decoder）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解码器包括： \n",
    "\n",
    "1. 输出嵌入（Output Embedding） \n",
    "2. 位置编码（Positional Encoding） \n",
    "3. N 个解码器层（decoder layers）\n",
    "\n",
    "目标（target）经过一个嵌入后，该嵌入和位置编码相加。该加法结果是解码器层的输入。解码器的输出是最后的线性层的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有难度吗？没有\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, \n",
    "             look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "        # 用来存注意力权重的\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, \n",
    "                                                   look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 26, 512) (64, 8, 26, 62)\n"
     ]
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=8000, \n",
    "                         maximum_position_encoding=5000)\n",
    "\n",
    "output, attn = sample_decoder(tf.random.uniform((64, 26)), \n",
    "                              enc_output=sample_encoder_output, \n",
    "                              training=False, \n",
    "                              look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "print(output.shape, attn['decoder_layer2_block2'].shape)\n",
    "# 输出：(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建 Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 包括编码器，解码器和最后的线性层。解码器的输出是线性层的输入，返回线性层的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
    "                               input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "        # 最后来个全连接层用于预测，这里不加softmax\n",
    "        # 其实tf最后输出是一般都不加softmax，而是损失函数里设置from_logits=True\n",
    "        # 官方解释是这样误差更小\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 26, 8000)\n"
     ]
    }
   ],
   "source": [
    "sample_transformer = Transformer(num_layers=2, d_model=512, \n",
    "                                 num_heads=8, dff=2048, \n",
    "                                 input_vocab_size=8500, \n",
    "                                 target_vocab_size=8000, \n",
    "                                 pe_input=10000, \n",
    "                                 pe_target=6000)\n",
    "\n",
    "temp_input = tf.random.uniform((64, 62))\n",
    "temp_target = tf.random.uniform((64, 26))\n",
    "\n",
    "fn_out, _ = sample_transformer(temp_input, \n",
    "                               temp_target, \n",
    "                               training=False, \n",
    "                               enc_padding_mask=None, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "print(fn_out.shape)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "# 输出：TensorShape([64, 26, 8000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 配置超参数（hyperparameters）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了让本示例小且相对较快，已经减小了num_layers、 d_model 和 dff 的值。\n",
    "\n",
    "Transformer 的基础模型使用的数值为：num_layers=6，d_model = 512，dff = 2048。\n",
    "关于所有其他版本的 Transformer，请查阅论文。\n",
    "\n",
    "Note：通过改变以下数值，可以获得在许多任务上达到最先进水平的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化器（Optimizer）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据论文中的公式，将 Adam 优化器与自定义的学习速率调度程序（scheduler）配合使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参照上面的公式就行，也没什么难以理解的地方\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  # tf.keras.optimizers.schedules里存着各种学习率，再也不用傻傻传入float型学习率或者自己写变化学习率了，有兴趣的可以看看手册\n",
    "  # 官方对tf.keras.optimizers.schedules.LearningRateSchedule的解释是A serializable learning rate decay schedule（一个可序列化的学习速率衰减时间表），可被传入各种优化器模块\n",
    "  # 继承这个类，就可以把自定义的学习率类传入优化器了\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Train Step')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEGCAYAAABGnrPVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de3xU5bX/8c9KQoAkQAgkELlfIhpviFG0Wm8VCx4t2tZWe9Ha9lBb+bU9bX8tnnN6O7+ec7SeVmtrtfbUVnuz9qJSxaLirbVaCaIIIpIMtwCSCZdIEiBA1u+PvQNjyGWSzGQmzPf9es1rZvZ+nr3XDCQrz97PXtvcHRERkUTJSnUAIiJydFFiERGRhFJiERGRhFJiERGRhFJiERGRhMpJdQCpNHLkSJ84cWKqwxAR6VeWLVtW5+7FHa3P6MQyceJEKisrUx2GiEi/YmYbOluvQ2EiIpJQSiwiIpJQSiwiIpJQSiwiIpJQSiwiIpJQSU0sZjbbzNaYWZWZLWhnvZnZ7eH6FWY2o6u+Znalma0ysxYzq2hnm+PNrMHMvpK8TyYiIh1JWmIxs2zgDmAOUA5cbWblbZrNAcrCxzzgzjj6rgTeDzzXwa5vBR5L3CcREZHuSOZ1LGcAVe4eATCz+4G5wOsxbeYC93lQu/9FMys0s1JgYkd93X11uOyIHZrZ5UAEaEzWh0q1ZRt2kJ2VxfRxhakORUSkXck8FDYG2BTzviZcFk+bePq+g5nlA18Dvt1Fu3lmVmlmldFotNMPkI4+cOcLXH7H8+g+OiKSrpKZWI4cUkDb34YdtYmnb1vfBm5194bOGrn73e5e4e4VxcUdViRISwdbDn8Fa7btTmEkIiIdS+ahsBpgXMz7scCWONvkxtG3rZnAB83su0Ah0GJme939Rz2IPS1t2bXn0OvHXnuL40YPTWE0IiLtS+aIZSlQZmaTzCwXuApY2KbNQuCacHbYmUC9u2+Ns+87uPu73X2iu08EbgP+62hKKgBV0WAwZgaPrdya4mhERNqXtMTi7geA+cBiYDXwgLuvMrPrzez6sNkigpPtVcBPgc911hfAzK4wsxrgLOBRM1ucrM+QbiLRYE7C/Aum8ua2BqpqOz3qJyKSEkmtbuzuiwiSR+yyu2JeO3BDvH3D5Q8CD3ax32/1INy0Vx1tYNjgAXxk5nh++FQVf1m5lfkXlqU6LBGRd9CV9/1IJNrA5OJ8SocNZsb4Qh5b+VaqQxIROYISSz8SiTYypbgAgEtOKmXVlreJRHU4TETSixJLP7F7735qd+9jcnE+AJedcgxm8NDyzSmOTETknZRY+onWE/etI5ZRQwdx9pSRPPjKZl0sKSJpRYmln6gOD3lNCUcsAFecOoZNO/awbMPOVIUlInIEJZZ+IhJtJDvLGF90OLHMPnE0gwdk8ycdDhORNKLE0k9E6hoYX5RHbs7hf7L8gTlcfMIoHl2xlX0HDqYwOhGRw5RY+onq2kYmj8w/YvkVp46hfs9+nlpdm4KoRESOpMTSDxxscdZtb2RKScER686ZOpLSYYO4f+mmdnqKiPQ9JZZ+YPPOPTQfaGl3xJKTncWHKsbx3Noom3Y0pSA6EZF3UmLpB6rrghlhk4uPHLEAfPj0cRhw/9KNfRiViEj7lFj6geraI6caxzqmcDAXTCvhgcoa9h9s6cvQRESOoMTSD0TqGhk2eABF+bkdtvnIzPFEd+9jyeptfRiZiMiRlFj6gUi0gSnF+Zi1d2PNwHnHFlM6bBC//ocOh4lIaimx9APV0cYOz6+0ysnO4qMzx/PXtXWs1W2LRSSFlFjS3Nt79xPdve9QjbDOfGTmBAbmZHHP8+v6IDIRkfYpsaS51uKTkzs4cR+rKD+X988Yyx9f3sz2hn3JDk1EpF1KLGku0k7xyc586pyJNB9o0bkWEUkZJZY0117xyc5MLRnCeccWc98LG1Q/TERSIqmJxcxmm9kaM6syswXtrDczuz1cv8LMZnTV18yuNLNVZtZiZhUxy2eZ2TIzey18vjCZn62vVEePLD7ZlU+/exJ1Dft0EzARSYmkJRYzywbuAOYA5cDVZlbeptkcoCx8zAPujKPvSuD9wHNttlUHXObuJwHXAr9M9GdKheB2xPGNVlqdM3UkJ40Zxo+fqeaALpgUkT6WzBHLGUCVu0fcvRm4H5jbps1c4D4PvAgUmllpZ33dfbW7r2m7M3df7u5bwrergEFmNjA5H61vtBaf7GqqcVtmxvwLp7JhexN/XrGl6w4iIgmUzMQyBogtuVsTLounTTx9O/MBYLm7HzE1yszmmVmlmVVGo9FubLLvdVZ8siuzjh/FtFFD+NFTVbS06NbFItJ3kplY2rtMvO1vuI7axNO3/Z2anQDcDHymvfXufre7V7h7RXFxcTybTJlDtyNup1x+V7KyglFLdbSRx1a+lejQREQ6lMzEUgOMi3k/Fmh7XKajNvH0PYKZjQUeBK5x9+oexJxWWhNLT0YsAJecVMrk4nx++NRajVpEpM8kM7EsBcrMbJKZ5QJXAQvbtFkIXBPODjsTqHf3rXH2fQczKwQeBW509+cT/WFSIVLXSGFe58UnO5OdZXzhPWW88dZunWsRkT6TtMTi7geA+cBiYDXwgLuvMrPrzez6sNkiIAJUAT8FPtdZXwAzu8LMaoCzgEfNbHG4rfnAVODrZvZK+ChJ1ufrC9W1DUwe2Xnxya5cdvIxlJcO5XuPv0nzAc0QE5HkM/fMPURSUVHhlZWVqQ6jQ6f/55Ocf2wxt1x5Sq+288yaWj7x86V8+30ncO27JiYmOBHJWGa2zN0rOlqvK+/TVGvxye5ONW7PeccWM3NSET98ai2N+w4kIDoRkY4psaSp7hSf7IqZ8bU5x1HX0Mz//lWVj0UkuZRY0tTh4pO9H7EAzBg/nEtOGs1dz1azZdeehGxTRKQ9SixpqjraEBafzEvYNm+cczwt7vzXotUJ26aISFtKLGkqEm1kQjeLT3ZlXFEe1583hUdWbOXFyPaEbVdEJJYSS5qqjjYk5PxKW589fwpjCgfzrYWrVKBSRJJCiSUNHWxx1tc1JWRGWFuDBmTz7/90PG+8tZtfvbgh4dsXEVFiSUM1O5toPtjS7XL58Zp94mjeXTaSWxav0Yl8EUk4JZY0dHiqceJHLBBMP/6vK06ixeHfH1pJJl8kKyKJp8SShqoTPNW4PeOK8vjKe6fx1Bu1/HnF1qTtR0QyjxJLGqqO9q74ZLw+8a6JnDKukG8vXMXOxuak7ktEMocSSxqKRBuSOlpplZ1l3PyBk6jfs5+vP6xDYiKSGEosaag62tjje7B013Gjh/Ivs47lkRVbefgVldYXkd5TYkkzb+/dT11DYopPxuv686ZQMWE4X39oJTU7m/psvyJydFJiSTOtM8KSNdW4PdlZxq0fno4DX3rgVQ7qbpMi0gtKLGmmuja8HXEfjlggmCX2rfedwEvrdnDXs/3+rs4ikkJKLGkmUtdATpYxYUTiik/G6wMzxnDpyaV87/E1qiUmIj2mxJJmqmsbGV+Ux4Dsvv+nMTNu+sDJTByZz/zfLKf27b19HoOI9H9KLGkmUpec4pPxKhiYw50fPY3GfQeY/9vlKlQpIt2W1MRiZrPNbI2ZVZnZgnbWm5ndHq5fYWYzuuprZlea2SozazGzijbbuzFsv8bM3pvMz5YMrcUn++Ials5MGz2E/7ziRF5at4NbHl+T0lhEpP9JWmIxs2zgDmAOUA5cbWblbZrNAcrCxzzgzjj6rgTeDzzXZn/lwFXACcBs4MfhdvqN1uKTqRyxtHr/jLF8dOZ4fvJshIeWb051OCLSjyRzxHIGUOXuEXdvBu4H5rZpMxe4zwMvAoVmVtpZX3df7e7t/Rk9F7jf3fe5+zqgKtxOv3F4qnFqRyytvnnZCZw5uYiv/nEFyzbsTHU4ItJPJDOxjAE2xbyvCZfF0yaevj3ZH2Y2z8wqzawyGo12scm+1Vp8sq+nGnckNyeLOz96GqXDBvGZX1bq4kkRiUsyE4u1s6ztlXcdtYmnb0/2h7vf7e4V7l5RXFzcxSb7VnW0keF9UHyyO4bn5/Kza09n34EWPn1vJQ37DqQ6JBFJc8lMLDXAuJj3Y4G2xag6ahNP357sL60FtyNOj9FKrKklBdzxkRmsrW3g+l8uY9+Bg6kOSUTSWDITy1KgzMwmmVkuwYn1hW3aLASuCWeHnQnUu/vWOPu2tRC4yswGmtkkggkBLyXyAyVbpA+LT3bXuccWc9P7T+JvVXV8WWVfRKQTOcnasLsfMLP5wGIgG7jH3VeZ2fXh+ruARcAlBCfam4DrOusLYGZXAD8EioFHzewVd39vuO0HgNeBA8AN7t5v/rSu3xMUn5xSkn4jllZXVoxjR2Mz//3YGxTl5/Lt952AWXtHIEUkkyUtsQC4+yKC5BG77K6Y1w7cEG/fcPmDwIMd9PlP4D97EXLKRFpP3KfpiKXVZ86bwvbGZu5+LkJRfi5fvOjYVIckImkmqYlF4ndoqnEaj1haLZh9HNsbmrntybUMyM7ihgumpjokEUkjSixpojoaFJ8cX9T3xSe7KyvL+O4HT+ZASwu3LF5DlhmfPX9KqsMSkTShxJImItHUFZ/siews43tXnoI73PyXN8iy4DCZiIgSS5pI16nGncnJzuL7HzqFFnf++7E3aHE0chGR+BKLmZ0DlLn7z82sGCgIy6ZIAhxscTZsb+LC40pSHUq35WRncduHp5Nlxs1/eYP6Pfv52uxpmi0mksG6TCxm9k2gApgG/BwYAPwKODu5oWWO1uKT6VIjrLtysrO49cPTGTo4h7ueraZ+TzPfufwksrOUXEQyUTwjliuAU4GXAdx9i5kNSWpUGeZwjbD0nmrcmews4//NPZHhebn88Kkq3t5zgO9/+BQG5vSrAtMikgDxJJZmd3czcwAz67+//dJUulU17ikz48sXT2PY4AF859HV1DXs4ycfP43CvPSpfSYiyRfPFKQHzOwnBCXt/xl4Evjf5IaVWaqjDQzPG8DwNCo+2RuffvdkfnDVdJZv3MUVP/476+oaUx2SiPShLhOLu/8P8AfgjwTnWb7h7rcnO7BMUh1t7Hczwroyd/oYfv3PM6nfs58rfvw8L0a2pzokEekjXSYWM7vZ3Z9w9//r7l9x9yfM7Oa+CC5TRKKNTOnH51c6cvrEIh783LsYkZ/Lx3/2Dx6o3NR1JxHp9+I5FDarnWVzEh1IpmotPnm0jVhaTRiRz58+ezZnTCriq39Ywb8/9JrK7osc5TpMLGb2WTN7DZhmZitiHuuAFX0X4tGttfhkfz9x35lheQO497oz+My5k/nVixv58E9eZGv9nlSHJSJJ0tmI5TfAZQT3Obks5nGau3+sD2LLCNXhjLD+PNU4HjnZWdx4yfHc+dEZrN22m0tv/xt/r65LdVgikgQdJhZ3r3f39e5+tbtvAPYQ3Oq3wMzG91mER7lIPyo+mQhzTirl4fnnMDw/l4/97z+47ck3OXCwJdVhiUgCxXPy/jIzWwusA54F1gOPJTmujFEdbWD8iP5TfDIRppYU8NANZ3P59DHc9uRarrr7RWp2NqU6LBFJkHh+m30HOBN4090nAe8Bnk9qVBkkuB3x0Xt+pSMFA3P4/oenc9uHp/PGW7uZ84O/8udXt6Q6LBFJgHgSy3533w5kmVmWuz8NTE9yXBnhwMEWNmxvYkrJ0X1+pTOXnzqGRZ9/N1NLCvg/v13Ol373CvVN+1Mdloj0QjyJZZeZFQDPAb82sx8Q3FNeeqlm556g+GQGjlhijR+RxwOfOYvPXziVh1/dwqxbn+XJ17elOiwR6aF4EstcoAn4F+AvQDXB7DDppUhdONU4g0csrQZkZ/Gli6fx0OfOpig/l0/fV8kX71/OzsbmVIcmIt0UT0mXRndvcfcD7n4vcAcwO56Nm9lsM1tjZlVmtqCd9WZmt4frV5jZjK76mlmRmT1hZmvD5+Hh8gFmdq+ZvWZmq83sxnhiTKXq2nCqcYaPWGKdNHYYC+efwxfeU8YjK7Yy69bneHTFVtw91aGJSJw6u0ByqJndaGY/MrOLwyQwH4gAH+pqw2aWTZCE5gDlwNVmVt6m2RygLHzMA+6Mo+8CYIm7lwFLwvcAVwID3f0k4DTgM2Y2sas4UylSd3QVn0yU3Jws/mXWsTw8/2xGDR3IDb95mWt/vpT1KmYp0i90NmL5JUHRydeATwOPE/zynuvuc+PY9hlAlbtH3L0ZuJ/gsFqsucB9HniRoIJyaRd95wL3hq/vBS4PXzuQb2Y5wGCgGXg7jjhTpjraeFRfcd9bJxwzjIdvOJtvXlbOyxt2cvFtz3Hbk2+yd79Kwoiks84Sy2R3/4S7/wS4muAukpe6+ytxbnsMEFt1sCZcFk+bzvqOcvetAOFz6/18/wA0AluBjcD/uPuOtkGZ2TwzqzSzymg0GudHSY5ItOGov+K+t3Kys7ju7Ek89eXzmH3CaG57ci3vve05nn6jVofHRNJUZ4nl0JxPdz8IrHP33d3Ydnv3pW37m6CjNvH0besM4CBwDDAJ+LKZTT5iI+53u3uFu1cUFxd3scnkqW/aT11Ds0YscSoZOojbrz6VX396JtlZxnW/WMo197zEG2+l9aBUJCN1llhOMbO3w8du4OTW12YWz09zDTAu5v1YoO0VcB216azvtvBwGeFzbbj8I8Bf3H2/u9cSXMRZEUecKVFd13o7YiWW7jh76kj+8oVz+cal5ayoqeeSH/yVG/+0gtrde1MdmoiEOqsVlu3uQ8PHEHfPiXk9NI5tLwXKzGySmeUCVxEUtIy1ELgmnBhwJlAfHt7qrO9C4Nrw9bXAw+HrjcCF4bbyCaoFvBFHnCkRyZDik8mQm5PFJ8+ZxLP/93yuO3sSf1hWwwW3PMMPl6ylqVmXWImkWtIKVLn7AWA+sBhYDTzg7qvM7Hozuz5stohgllkV8FPgc531DfvcBMwK65fNCt9DMIusAFhJkJh+7u5pW96/OsOKTyZDYV4uX7+0nMf/5TzOKRvJ9554k3O/+zQ/+9s6neAXSSHL5BOgFRUVXllZmZJ9f+aXlaytbeCpL5+fkv0fjZZt2Mn3n1jD81XbGT10EPMvnMqHKsaRm5M5BT5F+oKZLXP3Dk816CcuRSKaapxwp00Yzq8/fSa/+eeZjB0+mH9/aCUXfu8ZHqjcxH6V5hfpM0osKXDgYAvrtzfq/EqSvGvKSH5//Vn84rrTGZ6Xy1f/sILzb3mG+15Yr0NkIn0gnvux7I6ZHdb62GRmD7Y3nVe6VrNzD/sPukYsSWRmnD+thIXzz+aeT1QwetggvvHwKs65+SnueLqKt/eqgrJIsuTE0eb7BFN9f0NwfclVwGhgDXAPcH6ygjtaVR+6z71GLMlmZlx43CgumFbCS+t28ONnqrll8Rrueqaaj581gWvfNZFRQwelOkyRo0o8iWW2u8+MeX+3mb3o7v9hZv+arMCOZoemGqv4ZJ8xM2ZOHsHMySNYubmeO5+p5s5nq7n7uQj/dHIpnzx7EqeMK0x1mCJHhXgSS4uZfYigZArAB2PWZe6Usl6ojjZQlJ+r4pMpcuKYYdzx0Rls3N7EL/6+ngcqN/HwK1uYMb6QT54zidknjCYng24VLZJo8fz0fBT4OMEV7tvC1x8zs8EE15pINwW3I9ZhsFQbPyKPb1xWzgs3Xsg3Lytne2Mz83+znHd/92nueLpKV/OL9JCuY0nBdSwV33mC9xw3ips/eHKf71s6drDFefqNWu55fh1/r95OTpYxq3wUV58xnnOmjiQrq70SdiKZp6vrWLo8FGZmxcA/AxNj27v7JxMRYKZpLT6pqcbpJzvLuKh8FBeVjyISbeC3L23kD8tqeGzlW4wrGsxVp4/nyoqxlAzRyX6RzsRzjuVh4K/AkwTVg6UXWotPaqpxeptcXMC//VM5X3nvNP6y8i1++9JGblm8hlufeJMLjyvhA6eN5YJpJbqqX6Qd8SSWPHf/WtIjyRDVta1VjTVi6Q8G5mQzd/oY5k4fQ3W0gftf2siDy7fw+OvbKMwbwPtOOYb3zxjLKWOHYaZDZSIQX2J5xMwucfdFSY8mA0TqGsnJMsap+GS/MyUcxXxt9nH8taqOP728md8t3cR9L2xgcnE+H5gxlstPHcOYwsGpDlUkpeJJLF8A/tXM9hHc/MsAj7N0vrQRiTYwYUQeAzSdtd/Kyc7igmklXDCthLf37mfRiq386eXN3LJ4DbcsXkPFhOH808mlXHJSqS6+lIzUZWJx9yF9EUimqI426uZeR5GhgwZw1RnjueqM8Wzc3sTDr2zm0de28u0/v85/PPI6p08s4tKTS5lzYinFQwamOlyRPtHhdGMzO87d3zCzGe2td/eXkxpZH+jr6cYHDrZw/Df+wqfOmcyCOcf12X6l763dtptHX9vKIyu2UlXbQJbBzEkjuOTkUmYdP4rRwzSSkf6rN9ONvwTMA77XzjoHLuxlbBlnU1h8Uifuj35lo4bwxVFD+OJFx/Lmtt088uoWHlmxla8/tJKvP7SSU8YO4+ITRjOrfBRlJQU68S9HFV0g2YcjliWrt/Gpeyv542fP4rQJRX22X0kP7s7a2gaeeH0bj7++jVc37QJgwog8Li4fxazy0Zw2YTjZuhBT0lyvL5AMN/IujrxA8r5eR5dhWqsaq/hkZjIzjh01hGNHDeGGC6ay7e29PPH6Np54fRu/+Pt6fvrXdQzPG8C5xxZz/rRizi0rZkSBzstI/xPPlfe/BKYAr3D4AkkHlFi6KRJtVPFJOWTU0EF87MwJfOzMCezeu59n34yyZHUtz70Z5eFXtmAGJ48ZxnnTSjh/WjGnjC3UaEb6hXhGLBVAuffgmJmZzQZ+AGQD/+vuN7VZb+H6S4Am4BOtkwI66mtmRcDvCEZQ64EPufvOcN3JwE+AoUALcLq7p00lweB2xDq/IkcaMmgAl558DJeefAwtLc5rm+t5Zk2UZ96s5YdPreX2JWsZnjeAd5cVc96xxZxTNlJTmSVtxZNYVhLc2GtrdzZsZtnAHcAsoAZYamYL3f31mGZzgLLwMRO4E5jZRd8FwBJ3v8nMFoTvv2ZmOcCvgI+7+6tmNoLgupu0UR1t4KLjR6U6DElzWVnGKeMKOWVcIV+4qIydjc08tzbKs2uiPPtmlIWvbgGCG8WdPXUk75oykrMmj2BY3oAURy4SiCexjAReN7OXgH2tC939fV30OwOocvcIgJndD8wFYhPLXOC+cDT0opkVmlkpwWiko75zOXzXynuBZ4CvARcDK9z91TC+7XF8tj6zq6mZ7Y3NTCnRiEW6Z3h+7qGyMi0tzutb3+bv1XU8X7Wd31fWcN8LG8iy4D4zZ00ZwdlTRnL6xCIG52anOnTJUPEklm/1cNtjgE0x72sIRiVdtRnTRd9R7r4VwN23mllJuPxYwM1sMVAM3O/u320blJnNI5hGzfjx43vwsXqmWneNlATIyjJOHDOME8cMY965U2g+0MKrNbt4vqqOv1dt556/reMnz0bIzc7i5LHDOH1SEWdMLOK0icMZOkgjGukbnSaW8JDU1939oh5su72zjG3P03TUJp6+beUA5wCnE5yvWRJOiVvyjo243w3cDcF04y62mTCRqIpPSuLl5mRx+sQiTp9YxBcvgqbmA7y0bgcvVG/npfU7+OlzEe58phozOG70UGZOCtqePmm4yv9L0nSaWNz9oJk1mdkwd6/v5rZrgHEx78cCW+Jsk9tJ321mVhqOVkoJ7mzZuq1n3b0OwMwWATOAdySWVInUNTIgW8UnJbnycnM4f1oJ508LBvJ7mg+yfNNOXlq3g6Xrd/C7pZv4xd/XAzBxRN6hpHTq+EKmFBfoZmaSEPEcCtsLvGZmTwCNrQvd/fNd9FsKlJnZJGAzcBXwkTZtFgLzw3MoM4H6MGFEO+m7ELgWuCl8fjhcvhj4qpnlAc3AecCtcXy+PlFd28D4IhWflL41ODebd00JTvAD7D/YwsrN9Sxdv4OX1u3k8de38ftlNQAMGZTD9HGFnDqukFPHD2f6uEJNjZceiSexPBo+usXdD5jZfIJf+NnAPe6+ysyuD9ffBSwimGpcRXD46rrO+oabvgl4wMw+BWwErgz77DSz7xMkNAcWuXu3406WSF2jbu4lKTcgO4tTxw/n1PHDmXcutLQ4kboGlm/cxfJNu1i+cRc/erqKlvAg8cQReWH7QqaPK+T40qH640i6pJIufVDSRcUnpT9p3HeA1zbXB8lm406Wb9pFdHcwIXRgThYnHDOUk8IJBCeOGUZZSQE5SjYZJRH3vC8D/hsoBw6d7XP3yQmJMAOo+KT0J/kDczhz8gjOnDwCCGqcbanfGySZjbt4raaePyyr4d4XNgBBsjm+NEg2J40ZxgljhnLsqCEa2WSweA6F/Rz4JsH5igsIDlfpDF83tN6OWIfCpD8yM8YUDmZM4WAuPfkYAA62OOvqGlm5uZ7XwseDyzfzyxeDZJObk8Xxo4dwwphhHF86lPLSIUwbPZSCgXGVJ5R+Lp5/5cHuvsTMzN03AN8ys78SJBuJQ6SuNbFoxCJHh+wsY2pJAVNLCrj81DFAcL5m/fZGXttcfyjh/PnVLfzmHxsP9RtflMdxo4dwfOlQji8NnscNz9NstKNMXLPCzCwLWBueUN8MlHTRR2JEoo2MyM+lME8zbOTolZVlTC4uYHJxAXOnB8nG3dm8aw9vbN3NG2+9zeqtu1n91ts8sXobrad383OzmTZ6CMeVDuX40qEcN3oIx5YMUYmafiyexPJFIA/4PPD/CA6HXZvMoI421dEGnV+RjGRmjB2ex9jheVxUfrhO3p7mg7y5bTert77NG28Fz4+0Gd0UDxnIsaMKKCsZQln4fOyoAv2B1g/Ec8/7pQDBkTC/LvkhHX0i0UZmlav4pEirwbnZhwpttnJ3ttbvZc1bu1lbu5s3tzWwtraB31duorH54KF2IwtaE04BZaOGHHou0jU3aSOeWWFnAT8DCoDxZnYK8Bl3/1yygzsatBaf1IhFpHNmxjGFgzmmcDAXHHf4aHvrrLQ3t+2malsDb27bzdraBv748mYa9h041G543gAmjcxncnEBk0bmM6U4n0kjC5gwIqSe0dEAABJ9SURBVI9BA1SQsy/FcyjsNuC9BFe8E5akPzepUR1FVHxSpHdiZ6VdMO2dCWdr/V7W1jawdttuInWNRKIN/HVtlD+E1QSC/jCmcHBw/mdkPpOL85k8soBJxfmUDh2kiQNJENfcP3ffFNyT65CDHbWVd2otPjmlRIlFJJFiRzjnHVv8jnUN+w6wvq6R6mgD6+oaiUQbidQ1sGz9jnccVhs0IIuJI/KZNDKfCSPymTAijwlFeYwfkUfpsMG6Y2cPxZNYNoX3vHczyyU4ib86uWEdPaqjYfHJ4YNTHYpIxigYmHOoMkAsd6d2975DiWZdtJFIXSNr3trNk6u3sf/g4UokudlZjB0+mPGHkk0+E4rymDAij3FFOrzWmXgSy/UEtwgeQ1BB+HFA51fiFIk2MGFEvkpeiKQBM2PU0EGMGjqIs6aMeMe6gy3O1vo9bNzexIYdTWzY3sTGHY1s2N7EsvU72R1zPgdg9NBBh5NOUR5jiwaHM+AGUzJkUEaPduKZFVYHfDR2mZl9keDci3ShOtqgK+5F+oHsrMNTo9/VZp27s6OxmQ07moLEs72JDTsa2bi9iWfejB6qpdYqJys4TDd2ePAYU5h36PXYojxGDRl4VP+x2dP6Cl9CiaVL+w+2sHFHE7PKR6c6FBHpBTNjRMFARhQMZMb44Ues37v/IJt37aFm5x5qdjZRs3MPm8PXz6yJUtsm8WRnGaXDBoXJJo8xha0JaDClhYMpHTaoXx9q62liydwxXjds2tHE/oOuUi4iR7lBA7KZUlzQ4dGJvfsPsrV+7xFJp2bnHv62to5tu/fSttD88LwBlA4bzDGFgxg9bNDh10MPLxuYk57Jp6eJJXNr7XdDpHWqsQ6FiWS0QQOymTQymH3WnuYDLWzZtYct9XvYumsvb729ly279oTJaA+VG3ayq2n/Ef1G5OdSWhgmnWGDGB0mn9JhwainZOjAlCSfDhOLme2m/QRigKY4xUHFJ0UkHrk5WUwcmc/EDhIPQFPzAbbW7+Wt+sNJZ2t98LxxexMvRraze++BI/oV5eeGExYGMjqcuDB62CCmjR7S7mG9ROgwsbj7kKTsMYNU16r4pIgkRl5uTqeH2yC4fuet+j1s2RUkoLfeDh7bwtcrN9dT19AMwPtOOabvE4v0XqROM8JEpO8UDMxhaskQppZ0PC5oPtBCtGFfh+sT4eid75YGqqONqhEmImklNyfrUImcZElqYjGz2Wa2xsyqzGxBO+vNzG4P168wsxld9TWzIjN7wszWhs/D22xzvJk1mNlXkvnZurKrqZkdKj4pIhkoaYnFzLKBO4A5QDlwtZmVt2k2BygLH/OAO+PouwBY4u5lwJLwfaxbgccS/oG6qbX4pA6FiUimSeaI5Qygyt0j7t4M3A/MbdNmLnCfB14ECs2stIu+c4F7w9f3Ape3bszMLgciwKpkfah4VYfFJzXVWEQyTTITyxhgU8z7mnBZPG066zvK3bcChM8lAGaWD3wN+HZnQZnZPDOrNLPKaDTarQ/UHREVnxSRDJXMxNLe1fltr4vpqE08fdv6NnCruzd01sjd73b3CnevKC4u7qxpr1Sr+KSIZKhkTjeuAcbFvB8LbImzTW4nfbeZWam7bw0Pm9WGy2cCHzSz7wKFQIuZ7XX3HyXk03RTRMUnRSRDJfPP6aVAmZlNCu/jchXhXShjLASuCWeHnQnUh4e3Ouu7ELg2fH0t8DCAu7/b3Se6+0SCApn/laqksv9gCxu2N+nmXiKSkZI2YnH3A2Y2H1gMZAP3uPsqM7s+XH8XsAi4BKgCmoDrOusbbvom4AEz+xSwEbgyWZ+hpzbtaOJAizO5k/IMIiJHq6Reee/uiwiSR+yyu2JeO3BDvH3D5duB93Sx32/1INyEaS0+qRGLiGQinVlOgtapxlNGKrGISOZRYkmCSLSRkQW5DMsbkOpQRET6nBJLElRHG5is0YqIZCglliSI1Kn4pIhkLiWWBNvZGBSf1DUsIpKplFgSrPWukRqxiEimUmJJMFU1FpFMp8SSYNXRBgZkG2NVfFJEMpQSS4JFoo0qPikiGU2//RKsOtrAFJ1fEZEMpsSSQPsPtrBxe5Nu7iUiGU2JJYFai0/qxL2IZDIllgRqnRGmqcYiksmUWBIoouKTIiJKLIlUHW1Q8UkRyXhKLAkUiTaq+KSIZDwllgSK1DUypUTnV0QksymxJEhr8UmNWEQk0ymxJEhr8UmNWEQk0yU1sZjZbDNbY2ZVZragnfVmZreH61eY2Yyu+ppZkZk9YWZrw+fh4fJZZrbMzF4Lny9M5mdrq7o2nGqsEYuIZLikJRYzywbuAOYA5cDVZlbeptkcoCx8zAPujKPvAmCJu5cBS8L3AHXAZe5+EnAt8MskfbR2Vdep+KSICCR3xHIGUOXuEXdvBu4H5rZpMxe4zwMvAoVmVtpF37nAveHre4HLAdx9ubtvCZevAgaZ2cBkfbi2qmsbmajikyIiSU0sY4BNMe9rwmXxtOms7yh33woQPpe0s+8PAMvdfV+Po++mSF2DrrgXESG5icXaWeZxtomnb/s7NTsBuBn4TAfr55lZpZlVRqPReDbZpdbik6oRJiKS3MRSA4yLeT8W2BJnm876bgsPlxE+17Y2MrOxwIPANe5e3V5Q7n63u1e4e0VxcXG3P1R7NobFJ1XVWEQkuYllKVBmZpPMLBe4CljYps1C4JpwdtiZQH14eKuzvgsJTs4TPj8MYGaFwKPAje7+fBI/1xEih25HrENhIiI5ydqwux8ws/nAYiAbuMfdV5nZ9eH6u4BFwCVAFdAEXNdZ33DTNwEPmNmngI3AleHy+cBU4Otm9vVw2cXufmhEkyzVYfFJjVhERJKYWADcfRFB8ohddlfMawduiLdvuHw78J52ln8H+E4vQ+6RSGvxycEqPikiormxCRCJNmq0IiISUmJJAN3nXkTkMCWWXtrR2MzOpv2aaiwiElJi6aXIoRP3GrGIiIASS6+1TjVW8UkRkYASSy9VRxvIzc5S8UkRkZASSy9VRxuZMCJPxSdFREL6bdhLkboGnbgXEYmhxNILrcUndeJeROQwJZZeaC0+qRGLiMhhSiy9UF2rqcYiIm0psfRCpC6caqwRi4jIIUosvVBd28DIgoEqPikiEkOJpRcidY06DCYi0oYSSy9EoppqLCLSlhJLDx0uPqkRi4hILCWWHmotPqkRi4jIOymx9FC1qhqLiLRLiaWHItHGsPhkXqpDERFJK0osPVQdbWTiyDyysyzVoYiIpJWkJhYzm21ma8ysyswWtLPezOz2cP0KM5vRVV8zKzKzJ8xsbfg8PGbdjWH7NWb23mR+tki0QfdgERFpR9ISi5llA3cAc4By4GozK2/TbA5QFj7mAXfG0XcBsMTdy4Al4XvC9VcBJwCzgR+H20m4/Qdb2LijiSklOr8iItJWMkcsZwBV7h5x92bgfmBumzZzgfs88CJQaGalXfSdC9wbvr4XuDxm+f3uvs/d1wFV4XYSbsP2oPikRiwiIkdKZmIZA2yKeV8TLounTWd9R7n7VoDwuaQb+8PM5plZpZlVRqPRbn2gWJecNJryY4b2uL+IyNEqmYmlvbPaHmebePr2ZH+4+93uXuHuFcXFxV1ssn1TSwr48UdP4/hSJRYRkbaSmVhqgHEx78cCW+Js01nfbeHhMsLn2m7sT0REkiyZiWUpUGZmk8wsl+DE+sI2bRYC14Szw84E6sPDW531XQhcG76+Fng4ZvlVZjbQzCYRTAh4KVkfTkRE2peTrA27+wEzmw8sBrKBe9x9lZldH66/C1gEXEJwor0JuK6zvuGmbwIeMLNPARuBK8M+q8zsAeB14ABwg7sfTNbnExGR9pl7V6cujl4VFRVeWVmZ6jBERPoVM1vm7hUdrdeV9yIiklBKLCIiklBKLCIiklBKLCIiklAZffLezKLAhl5sYiRQl6BwEklxdY/i6h7F1T1HY1wT3L3DK8wzOrH0lplVdjYzIlUUV/coru5RXN2TiXHpUJiIiCSUEouIiCSUEkvv3J3qADqguLpHcXWP4uqejItL51hERCShNGIREZGEUmIREZGEUmLpATObbWZrzKzKzBb00T7Xm9lrZvaKmVWGy4rM7AkzWxs+D49pf2MY3xoze2/M8tPC7VSZ2e1m1t4N0jqL4x4zqzWzlTHLEhZHeNuD34XL/2FmE3sR17fMbHP4nb1iZpekIK5xZva0ma02s1Vm9oV0+M46iSul35mZDTKzl8zs1TCub6fJ99VRXOnwfyzbzJab2SPp8F0B4O56dONBUMa/GpgM5AKvAuV9sN/1wMg2y74LLAhfLwBuDl+Xh3ENBCaF8WaH614CziK44+ZjwJxuxnEuMANYmYw4gM8Bd4WvrwJ+14u4vgV8pZ22fRlXKTAjfD0EeDPcf0q/s07iSul3Fm6jIHw9APgHcGYafF8dxZUO/8e+BPwGeCRtfh6780tFDyf88hfHvL8RuLEP9rueIxPLGqA0fF0KrGkvJoL72pwVtnkjZvnVwE96EMtE3vkLPGFxtLYJX+cQXBlsPYyrox/6Po2rzb4fBmaly3fWTlxp850BecDLwMx0+r7axJXS74vgTrlLgAs5nFhS/l3pUFj3jQE2xbyvCZclmwOPm9kyM5sXLhvlwR03CZ9LuohxTPi67fLeSmQch/q4+wGgHhjRi9jmm9kKCw6VtR4SSElc4WGEUwn+2k2b76xNXJDi7yw8tPMKwW3Hn3D3tPi+OogLUvt93QZ8FWiJWZby70qJpfvaOyfRF3O2z3b3GcAc4AYzO7eTth3F2Nex9ySORMZ4JzAFmA5sBb6XqrjMrAD4I/BFd3+7s6Z9GVs7caX8O3P3g+4+neCv8TPM7MTOPkKK40rZ92VmlwK17r6sq9j7KqZWSizdVwOMi3k/FtiS7J26+5bwuRZ4EDgD2GZmpQDhc20XMdaEr9su761ExnGoj5nlAMOAHT0Jyt23hb8MWoCfEnxnfR6XmQ0g+OX9a3f/U7g45d9Ze3Gly3cWxrILeAaYTRp8X+3FleLv62zgfWa2HrgfuNDMfkUafFdKLN23FCgzs0lmlktwQmthMndoZvlmNqT1NXAxsDLc77Vhs2sJjpMTLr8qnNExCSgDXgqHxbvN7Mxw1sc1MX16I5FxxG7rg8BTHh7g7a7WH67QFQTfWZ/GFW7nZ8Bqd/9+zKqUfmcdxZXq78zMis2sMHw9GLgIeCMNvq9240rl9+XuN7r7WHefSPB76Cl3/1iqv6vW4PTo5gO4hGAWTTXwb32wv8kEszleBVa17pPgWOcSYG34XBTT59/C+NYQM/MLqCD4z18N/Ijun+T9LcGQfz/BXzOfSmQcwCDg90AVwUyVyb2I65fAa8CK8AekNAVxnUNw6GAF8Er4uCTV31kncaX0OwNOBpaH+18JfCPR/9cTHFfK/4+Ffc/n8Mn7lP88qqSLiIgklA6FiYhIQimxiIhIQimxiIhIQimxiIhIQimxiIhIQimxiHSTmY2ww9Vs37J3VrfNjXMbPzezad3YZ6mZLbKguu7rZrYwXD7ZzK7q6WcRSQZNNxbpBTP7FtDg7v/TZrkR/Hy1tNux+/v5GfCyu98Rvj/Z3VeY2UXAfHe/PBH7EUkEjVhEEsTMpprZSjO7i6D6bamZ3W1mlRbcw+MbMW3/ZmbTzSzHzHaZ2U3haOQFMytpZ/OlxBQKdPcV4cubgAvC0dLnw+1934J7h6wws0+H+7vIgvuvPBSOeO4Ik59IwimxiCRWOfAzdz/V3TcT3BejAjgFmGVm5e30GQY86+6nAC8An2ynzY+Ae83sKTP715hSIguAp919urvfDswjKEx4BnA6QcHS8WHbmcAXgZOA44G5CfnEIm0osYgkVrW7L415f7WZvUwwgjmeIPG0tcfdHwtfLyO4r8w7uPsigiq6Pwu3sdzM2itffjFwnQXl3f8BFBLUhAJ40d3Xu/tBgqKF53T3w4nEIyfVAYgcZRpbX5hZGfAF4Ax33xVWnh3UTp/mmNcH6eDn0t23A78Gfm1mfyFIDI1tmhnwOXdf8o6FwbmYtidUdYJVkkIjFpHkGQrsBt4OD129t4v2HTKz94RVdTGzoQS3lt0Ybn9ITNPFwOcsKHGOmU1r7QecaWbjzSwb+BDwt57GI9IZjVhEkudl4HWCqrER4PlebOt04Edmtp/gD8I73X15OL0528xeJThMdgcwHnglPDdfy+FzKX8nuBHVCQT3E0nq7R4kc2m6sUgG0LRk6Us6FCYiIgmlEYuIiCSURiwiIpJQSiwiIpJQSiwiIpJQSiwiIpJQSiwiIpJQ/x+j1WMZO1g2pAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")\n",
    "# 自己跑一下就会发现，这是个先上升再下降最后平缓的学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数与指标（Loss and metrics）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于目标序列是填充（padded）过的，因此在计算损失函数时，应用填充遮挡非常重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    # 等于0的地方，mask就是False，转为数字就是0了\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    # 乘上mask，序列为0的地方就不计入损失函数了\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "# 有sparse就用未经softmax的模型输出，老规矩了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练与检查点（Training and checkpointing）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    # 用于编码器的mask\n",
    "\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "    # 用于解码器第二个注意力模块的mask，和上面的mask一样，用不同的名字区分一下\n",
    "\n",
    "    # 用于解码器第一个注意力模块的mask，这个mask由两个mask复合而成：\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    # 1、前瞻mask，tar即目标数据，维度是batch_size × tar_seq_len × embedding_dims\n",
    "    # 可以回忆下create_look_ahead_mask函数，生成一个上三角阵，维度是tar_seq_len × tar_seq_len，对角线及其以下都是0，其他的是1\n",
    "    # 对于tar的一条数据来说，第i个时间步，对应前瞻mask的第i行，i及i前面的mask是0，后面的是1（表示遮挡），so easy\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    # 2、填充mask\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "    # 用maximum函数，这样两个mask只要有一个是1的地方，combined mask对应位置就是1\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建检查点的路径和检查点管理器（manager）。这将用于在每 n 个周期（epochs）保存检查点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# 如果检查点存在，则恢复最新的检查点。\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目标（target）被分成了 tar_inp 和 tar_real。tar_inp 作为输入传递到解码器。tar_real 是位移了 1 的同一个输入：在 tar_inp 中的每个位置，tar_real 包含了应该被预测到的下一个标记（token）。\n",
    "\n",
    "例如，sentence = “SOS A lion in the jungle is sleeping EOS”\n",
    "\n",
    "tar_inp = “SOS A lion in the jungle is sleeping”\n",
    "\n",
    "tar_real = “A lion in the jungle is sleeping EOS”\n",
    "\n",
    "Transformer 是一个自回归（auto-regressive）模型：它一次作一个部分的预测，然后使用到目前为止的自身的输出来决定下一步要做什么。\n",
    "\n",
    "在训练过程中，本示例使用了 teacher-forcing 的方法（就像文本生成教程中一样）。无论模型在当前时间步骤下预测出什么，teacher-forcing 方法都会将真实的输出传递到下一个时间步骤上。\n",
    "\n",
    "当 transformer 预测每个词时，自注意力（self-attention）功能使它能够查看输入序列中前面的单词，从而更好地预测下一个单词。\n",
    "\n",
    "为了防止模型在期望的输出上达到峰值，模型使用了前瞻遮挡（look-ahead mask）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "# 该 @tf.function 将追踪-编译 train_step 到 TF 图中，以便更快地执行。该函数专用于参数张量的精确形状。\n",
    "# 为了避免由于可变序列长度或可变批次大小（最后一批次较小）导致的再追踪，使用 input_signature 指定更多的通用形状。\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                     True, \n",
    "                                     enc_padding_mask, \n",
    "                                     combined_mask, \n",
    "                                     dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "葡萄牙语作为输入语言，英语为目标语言："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # inp -> portuguese, tar -> english\n",
    "    for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "    if batch % 50 == 0:\n",
    "        print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                             ckpt_save_path))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估（Evaluate）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面的是原注释，可以不看，直接看代码注释。\n",
    "以下步骤用于评估：\n",
    "\n",
    "用葡萄牙语分词器（tokenizer_pt）编码输入语句。此外，添加开始和结束标记，这样输入就与模型训练的内容相同。这是编码器输入。\n",
    "解码器输入为 start token == tokenizer_en.vocab_size。\n",
    "计算填充遮挡和前瞻遮挡。\n",
    "解码器通过查看编码器输出和它自身的输出（自注意力）给出预测。\n",
    "选择最后一个词并计算它的 argmax。\n",
    "将预测的词连接到解码器输入，然后传递给解码器。\n",
    "在这种方法中，解码器根据它预测的之前的词预测下一个。\n",
    "Note：这里使用的模型具有较小的能力以保持相对较快，因此预测可能不太正确。要复现论文中的结果，请使用全部数据集，并通过修改上述超参数来使用基础 transformer 模型或者 transformer XL。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    start_token = [tokenizer_pt.vocab_size]\n",
    "    end_token = [tokenizer_pt.vocab_size + 1]\n",
    "\n",
    "    # 输入语句是葡萄牙语，增加开始和结束标记\n",
    "    inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "    encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "\n",
    "    # 因为目标是英语，输入 transformer 的第一个词应该是英语的开始标记。\n",
    "    decoder_input = [tokenizer_en.vocab_size]\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(MAX_LENGTH):\n",
    "    # 预测的时候有点不一样\n",
    "    # 逻辑上就是，第一轮循环，只有一个起始符，其他的0，然后取预测结果的最后一个词，和起始符拼接送入下一轮\n",
    "    # 注意这里取预测结果的最后一个词，就像RNN文本生成，每一轮也是取最后一个词\n",
    "    # 再取预测结果的最后一个词，和上一轮的输入拼接，送入下一轮\n",
    "    # 如此循环，直到预测结果最后一个词是终止符\n",
    "\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "            encoder_input, output)\n",
    "\n",
    "        # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "    \n",
    "        # 从 seq_len 维度选择最后一个词\n",
    "        predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # 如果 predicted_id 等于结束标记，就返回结果\n",
    "        if predicted_id == tokenizer_en.vocab_size+1:\n",
    "            return tf.squeeze(output, axis=0), attention_weights\n",
    "        # 连接 predicted_id 与输出，作为解码器的输入传递到解码器。\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画图，没啥好说的，主体逻辑都在上方\n",
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "    sentence = tokenizer_pt.encode(sentence)\n",
    "\n",
    "    attention = tf.squeeze(attention[layer], axis=0)\n",
    "  \n",
    "    for head in range(attention.shape[0]):\n",
    "        ax = fig.add_subplot(2, 4, head+1)\n",
    "\n",
    "        # 画出注意力权重\n",
    "        ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "        fontdict = {'fontsize': 10}\n",
    "\n",
    "        ax.set_xticks(range(len(sentence)+2))\n",
    "        ax.set_yticks(range(len(result)))\n",
    "\n",
    "        ax.set_ylim(len(result)-1.5, -0.5)\n",
    "\n",
    "        ax.set_xticklabels(\n",
    "            ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "            fontdict=fontdict, rotation=90)\n",
    "\n",
    "        ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                            if i < tokenizer_en.vocab_size], \n",
    "                           fontdict=fontdict)\n",
    "\n",
    "        ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "    result, attention_weights = evaluate(sentence)\n",
    "\n",
    "    predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                              if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(predicted_sentence))\n",
    "\n",
    "    if plot:\n",
    "        plot_attention_weights(attention_weights, sentence, result, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"este é um problema que temos que resolver.\")\n",
    "print (\"Real translation: this is a problem we have to solve .\")\n",
    "\n",
    "translate(\"os meus vizinhos ouviram sobre esta ideia.\")\n",
    "print (\"Real translation: and my neighboring homes heard about this idea .\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

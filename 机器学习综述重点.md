# 机器学习综述重点

## 机器学习算法的最终目的

我们经常说预测目标属于某个类别的概率

这句话背后的目标是：预测值与正确值的距离足够小，则预测值准确率越高。

而距离这种概念又经常被归纳为损失，在信息学中，又经常被称之为熵。

损失函数，如MSE，即均方误差，经常作为回归学习或对比学习的损失函数，或者Cross-Entropy，即交叉熵，经常作为分类任务的损失函数，又称之为目标函数，因为尽可能逐渐降低或减小训练过程中，训练数据预测值与正例的误差，亦即距离，就是绝大部分机器学习的算法本质。



## 传统机器学习

亦即我们经常说的应用统计，大部分算法封装在Scikit-Learn这个python库

目前传统机器学习最常用的算法有：随机森林，K-均值聚类等，对于数据量较小，且任务为简单有监督分类或无监督聚类的情况，可以不使用昂贵的深度学习算法。

除此之外，XGBOOST这种集成学习模型也经常会用到，不过需要安装额外的python包。

之前我们经常会用的SVM算法，隐马尔科夫模型等等，已经有更好的深度学习算法可以代替。

## 深度学习

### CNN

亦即卷积神经网络，在之前几乎等价于深度学习本身。

CNN，广泛运用在CV以及NLP领域

最方便实惠的CNN的Backbone模型，是ResNet50，拿这个入门深度学习，可以的

所谓的Backbone，就是骨干网络的意思，意思是，如果做复杂任务，比如图像目标识别，图像分割，编码器部分可以原封不动使用Backbone，如ResNet50，解码器部分可以自己桥接后续网络层，比如U-NET



### RNN

可以不用理会了，过气了



### Transformer

超级hot，2019年开始火爆，因为基于Transformer编码器出现的Bert，使得该基础框架统一了NLP领域

2020年，ViT，亦即Vision Transformer的提出，将图片可以分割为一片片16x16的patches，每个patch相当于NLP单词对应的token，于是：Transformer统一了NLP与CV领域的应用。

### 个人及小公司使用局限性

尽管Transformer看起来特别局气，但是目前为止统一江湖的还是Bert。

因为Transformer本身需要数据量特别大，训练Epoch足够多，才能达到较好的效果，在一般数量，是指不超过一百万数量级，的训练数据面前，实际效果不一定比卷积神经网络强。

Bert之所以统一NLP了，因为我们拿到的backbone，已经是被互联网大爹Google他们训练完成的了，NLP领域，初始无监督的训练集里，什么单词没见过，初始特征就很强啊！

所以我们只需要finetune，直接架上我们自己的少得可怜的数据训练即可，所以感觉没有那么难训练。

但是CV领域，目前ViT相关的预训练模型，还只是ImageNet数据，图像特征偏研究领域。

而各家公司的图像领域业务各不相同，这就是直接用ViT做pre-train模型，然后架上自家少的可怜的数据，finetune训练效果往往一般，甚至还不如ResNet，EfficientNet这些CNN传统骨干网络的原因。



